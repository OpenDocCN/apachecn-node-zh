Beginning Your Microservice Journey <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 开始你的微服务旅程

微服务是企业中制作快速、有效和可伸缩应用程序的最切实可行的解决方案之一。 然而，如果它们没有被正确地设计或理解，错误的实现和解释可能会导致灾难性的或不可恢复的失败。 本章将开始我们的微服务之旅，让我们亲自动手，深入实践。

本章将以描述购物微服务开始，我们将在整个旅程中发展。 我们将学习如何将一个系统分割成一组连接的微服务。 我们将设计购物车微服务的总体架构，定义分离层，添加缓存级别，等等。

本章将涵盖以下主题:

*   购物车微服务概述
*   购物车微服务的架构设计
*   购物车微服务实施方案
*   模式设计和数据库选择
*   Microservice 会展方面
*   开发一些购物车微服务
*   微服务设计最佳实践

# 购物车微服务概述

在开发新系统时，最重要的方面是它的设计。 糟糕的初始设计总是未来更多挑战的主要原因。 与其事后抱怨，解决错误，或者用补丁来掩盖糟糕的设计，不如不要匆忙完成设计过程，花足够的时间，并拥有一个灵活的万无一无的设计。 这只能通过清楚地理解需求来实现。 在本节中，我们将简要概述购物车微服务; 我们需要通过微服务解决的问题; 以及业务流程、功能视图、部署和设计视图的概述。

# 业务流程概述

我们场景的用例非常简单。 下面的流程图显示了我们需要转换为微服务的端到端购物流程。 用户将商品添加到购物车中，更新库存，用户为商品付款，然后可以结账。 根据业务规则，涉及多个验证。 例如，如果用户支付失败，那么他们就不能结账; 如果库存不可用，那么该物品就不应该添加到购物车中等等。 看看下面的图表:

![](assets/72153c1f-f798-4aa4-b08b-7663f60d4e55.png)

Business process overview

# 功能视图

每个业务功能及其子功能都显示在一行中，这实质上构成了购物车微服务。 一些子功能涉及到多个业务功能，因此我们需要管理一些横切关注点。 例如，库存服务既可以作为一个单独的流程使用，也可以作为一个人检出产品时使用。 下图展示了购物车微服务的功能视图:

![](assets/193b2e4a-8d72-423a-8488-704770f5a066.png)

Functional view

该图表将业务功能合并到一个图中。 例如，库存服务状态有两个子功能—添加产品详细信息和添加产品数量和库存项目。 这总结了库存服务的目标。 为我们的系统创建一个功能视图可以让我们清楚地了解所有的业务流程以及其中涉及的相关事物。

# 部署视图

部署的需求非常简单。 根据需求，我们需要添加新的服务来动态地支持各种业务功能。 例如，现在的支付媒介是**PayPal**，但将来我们可能还需要支持一些本地支付选项，比如银行钱包。 到那时，我们应该能够很容易地添加新的微服务，而不会破坏整个生态系统。 下图显示了部署视图。 目前，有两个节点(一个主节点和一个从节点)，但根据需求，节点数量可能会根据业务能力、流量峰值和其他需求增加或减少:

![](assets/35bb0ad2-fda8-42fe-b101-a7b43af015f7.png)

Deployment view

在本节中，我们简要介绍了购物车微服务系统。 我们了解了它的功能、业务流程和部署视图。 在下一节中，我们将看到购物车微服务的架构设计。

# 系统架构设计

在本节中，我们将研究分布式微服务中涉及的体系结构方面。 我们将查看我们的整体架构图，我们将在整本书中使用它，并查看一些方面，如分离关注点、如何应用响应式模式和微服务效率模型。 让我们开始吧。

现在我们知道了我们的业务需求，让我们设计我们的架构。 基于我们对[第 1 章](01.html)、*揭密微服务*中其他概念的了解，我们有了最终的总体图，如下所示:

![](assets/71920699-b87a-49d8-a2ca-fd95f8fa8931.png)

Microservice architecture We will study components such as API Gateway, service registry, and discovery in much more detail in later chapters. Here, they are just mentioned as part of the overall view.

让我们理解前面图表中的关键组件，以便更好地了解我们的体系结构。

# 不同 microservices

如果我们正确理解了我们的业务需求，我们将会得到以下业务能力:

*   产品目录
*   价格目录
*   折扣
*   发票
*   付款
*   库存

根据我们的业务能力和单一职责，我们将微服务简单地划分为各种较小的应用程序。 在我们的设计中，我们确保每个业务功能是由单个微服务实现的，并且我们不会使用多个微服务使一个微服务超载。 我们将整个系统简单地划分为各种微服务，如购物车微服务、产品微服务、支付微服务、消费者微服务、缓存微服务、价格计算和建议微服务等。 整体的粒状流可以在前面的图中看到。 另一个需要注意的重要事情是，每个微服务都有其独立的数据存储。 不同的业务功能有不同的需求。 例如，当一个人签出一个产品时，如果交易失败，那么所有的交易，比如将产品添加到客户的购买项目中，从产品库存中扣除数量，等等，都应该回滚。 在这种情况下，我们需要能够处理事务的关系数据库，而在产品的情况下，我们的元数据不断变化。 一些产品可能比其他产品有更多的功能。 在这种情况下，使用固定的关系模式是不明智的，我们可以使用 NoSQL 数据存储。

在写这本书的时候，MongoDB 4.0 还没有被引入。 它提供了以下事务性和 NoSQL 优点。

# 缓存 microservice

下一个我们要讲的组件是集中缓存存储。 这个微服务直接与所有微服务交互，我们可以在需要的时候使用这个服务来缓存我们的响应。 通常情况下，服务宕机了，我们仍然可以通过显示缓存的数据来保存应用程序(产品信息和元数据等内容很少改变; 我们可以将它们缓存一段时间，从而防止额外的数据库跳转)。 拥有缓存可以提高系统的性能和可用性，这最终会导致成本优化。 它提供了快速的用户体验。 由于微服务不断移动，它们往往无法被触及。 在这种情况下，当访问可用性区域失败时，缓存响应总是有利的。

# 服务注册和发现

在图的开始，我们包含了服务注册中心。 这是一个动态数据库，用于维护所有微服务的启动和关闭事件。 服务订阅注册中心并侦听更新，以了解服务是否停止。 整个过程是通过服务注册中心和发现完成的。 当服务下降或上升时，注册程序更新注册表。 该注册中心缓存在所有订阅该注册中心的客户端上，因此每当需要与服务进行交互时，都会从该注册中心获取一个地址。 我们将在[第 6 章](06.html)、*服务注册和发现*中详细介绍这个过程。

# Registrator

我们将要研究的下一个组件是**注册器**([http://gliderlabs.github.io/registrator/latest/](http://gliderlabs.github.io/registrator/latest/))，它与缓存一起可用。 注册器是一个第三方服务注册工具，它主要监视微服务的启动和关闭事件，并根据这些事件的输出动态更新集中的服务注册表。 然后，不同的服务可以直接与注册中心通信，以获得更新的服务位置。 注册者确保注册和注销代码不会在系统间重复。 我们将在[第 6 章](06.html)、*服务注册和发现*中更详细地了解这一点，其中我们集成了注册器和领事。

# Logger

任何应用程序的一个重要方面是日志。 当使用适当的日志时，分析任何问题都变得非常容易。 因此，这里我们有一个集中的日志微服务，它基于著名的 Elastic 框架。 Logstash 监视日志文件，并在推送到 Elasticsearch 之前将它们转换为适当的 JSON。 我们可以通过 Kibana 仪表盘看到这些日志。 每个微服务都将配置其惟一的 UUID 或一些日志模式。 我们将在[第 9 章](09.html)、*部署、日志记录和监控*中更详细地讨论这个问题。

# 网关

这是最重要的部分，也是我们微服务的起点。 它是我们处理横切关注点的中心点，例如身份验证、授权、转换等等。 在不同的服务器上创建不同的微服务时，我们通常从客户机提取主机和端口的信息。 客户机只向网关发出一个请求，网关通过与服务注册中心和负载平衡器交互并将请求重定向到适当的服务来处理其余的事情。 这是微服务中最重要的部分，它应该提供高可用性。

在浏览完体系结构图之后，现在让我们了解与我们稍后将使用的体系结构相关的一些方面。

# 参与设计方面

在实际编码之前，我们需要了解*如何*和*为什么*。 比方说，如果我必须砍一棵树(PS:我是一个自然爱好者，我不支持这一点)，而不是直接砍倒它，我宁愿先磨好斧头。 我们也要这样做，先磨利我们的斧子。 在本节中，我们将研究设计微服务时涉及的各个方面。 我们将探讨通过什么样的传播模式，微服务包含哪些内容，以及为了实现高效的微服务发展需要关注哪些领域。

# Microservice 效率模型

基于不同的需求和要求，我们定义了一个微服务效率模型。 任何适当的微服务实现都必须遵循它，并提供一组标准的功能，如下所示:

*   通过 HTTP 和 HTTP 监听器进行通信
*   消息或套接字监听器
*   存储功能
*   正确的业务/技术能力定义
*   服务端点定义和通信协议
*   服务接触
*   安全服务
*   通过 Swagger 等工具编写服务文档

在下面的图表中，我们总结了我们的微服务效率模型:

![](assets/45ff4392-2534-40b5-9a86-b3c583c80c30.png)

Microservice efficiency model

现在让我们看看上述图的四个部分。

# 核心功能

核心功能是微服务本身的一部分。 它们包括以下功能:

*   **技术功能**:这里涉及到所需的任何技术功能，例如与服务注册中心交互、将事件发送到事件队列、处理事件等等。
*   **业务能力**:为实现业务能力或满足业务需求而编写的微服务。
*   **HTTP 侦听器**:技术能力的一部分; 这里我们为外部消费者定义 api。 在启动服务器时，将启动一个 HTTP 侦听器，从而消除任何其他需求。
*   消息侦听器:基于事件的通信的一部分，其中发送方不担心消息侦听器是否被实现。
*   **API 网关**:终端客户端的一个通信点。 API 网关是处理任何核心问题的唯一地方。
*   **文档存储或数据存储**:应用程序的数据层。 根据我们的需要，我们可以使用任何可用的数据存储。

# 支持效率

这些是帮助实现核心微服务实现的解决方案。 它们包括以下组成部分:

*   **负载均衡器**:一个基于服务器拓扑变化的**应用负载均衡器。 它处理动态服务的上升或下降。
    **服务注册中心**:一个运行时环境，用于服务向上或向下发布。 它维护所有服务和可用实例的活动日志。
    **中央日志**:核心集中式日志解决方案，将日志观察到所有地方，而不是单独打开容器在那里寻找日志。
    **Security**:通过常见的可用机制(如 OAuth、基于令牌、基于 ip 等)检查可信的客户端请求。
    **Testing**:测试微服务和基本要素，如微服务间的通信、可伸缩性等。**

 **# 基础设施的作用

以下是高效实现微服务所需的基础设施期望:

*   **服务器层**:部署微服务时选择的有效机制。 众所周知的选项包括 Amazon EC2 实例、Red Hat OpenShift 或无服务器。
*   **容器**:Dockerizing 应用程序，所以它可以轻松地在任何操作系统上运行，而无需太多的麻烦安装。
*   **CI/CD**:维护简单部署周期的过程。
*   **集群**:服务器负载平衡器根据需要处理应用程序中的负载或峰值。

# 治理

简化应用程序开发的整体生命周期的流程和参考信息包括以下内容:

*   **契约测试**:测试对微服务的期望和实际输出，以确保频繁的更改不会破坏任何东西
*   **可扩展性**:生成新的实例，并根据需要移除这些实例，以处理负载峰值
*   **文档**:生成文档以方便理解某人实际在做什么

在下一节中，我们将为微服务的发展制定一个实施计划。

# 购物车微服务实施方案

微服务开发的关键挑战之一是修复微服务的范围:

*   如果一个微服务太大，你最终会陷入巨大的困境，陷入巨大的周转时间，难以添加新功能和实现 bug 修复
*   如果微服务太小，要么服务之间的紧密耦合，要么代码重复和资源消耗过多
*   如果微服务的大小是正确的，但是有边界的上下文不是固定的，例如服务共享数据库，它会导致更高的耦合和依赖

在本节中，我们将为购物车微服务设计一个实现计划。 我们将制定一个通用的工作流程或计划，并根据计划设计我们的系统。 我们也会看到，当我们的范围不清楚时，我们应该做什么，以及在这种情况下如何进行，最终达到我们的微服务目标。 我们将研究如何潜在地避免上述循环漏洞。

# 当范围不清楚时该怎么办

到目前为止，我们已经设计了基于范围划分的微服务的架构计划，但那时我们的需求已经非常清楚了。 我们很清楚我们要做什么。 但在大多数情况下，您不会遇到类似的情况。 你要么被从一个系统迁移到 microservices 否则你将会陷入不断变化的需求或业务功能仍在继续发展,或者它可能是技术能力无法估计的复杂性在入门阶段,使你很难 microservices 范围。 本节针对以下场景，您可以执行以下操作:

1.  :决定微服务的范围始终是一项艰巨的任务，因为它定义了整体的有限上下文。 如果没有明确的决定，我们最终会陷入巨大的地狱。 然而，如果范围缩小太多，它也有它的缺点。 您将会遇到困难，因为最终会在两个微服务之间出现数据复制、责任不明确以及独立部署服务的困难。 从现有的微服务中分离出微服务要比管理过于狭窄的微服务容易得多。
2.  **将微服务从现有的微服务中分离出来**:一旦你觉得一个微服务太大了，你就需要开始分离服务。 首先，需要根据业务和技术能力来决定现有和新的微服务的范围。 任何与新微服务相关的内容都有自己的模块。 然后，现有模块之间的任何通信都转移到公共接口，例如 HTTP API/基于事件的通信，等等。 微服务也可以为以后的发展做规划; 当有疑问时，总是创建一个单独的模块，这样我们就可以很容易地把它移出来。

3.  **确定技术能力**:技术能力是任何支持其他微服务的东西，例如监听事件队列发出的事件、注册到服务注册中心等等。 将技术能力保留在相同的微服务中可能是一个巨大的风险，因为它很快就会导致紧密耦合，而相同的技术能力也可能被许多其他服务实现。
4.  **遵循基于业务和技术能力的微服务标准**:微服务遵循固定的标准——自给自足、弹性、透明、自动化和分发。 每一点都可以简单地表述为:
    *   微服务服务于单一的业务功能(模块化是关键)。
    *   可以很容易地单独部署微服务。 每个服务都有自己的构建脚本和 CI/CD 管道。 公共点是 API 网关和服务注册中心。
    *   你可以很容易找到微服务的所有者。 它们将被分发，每个团队可以拥有一个微服务。
    *   更换微服务不需要太多麻烦。 我们将通过服务注册中心和发现提供通用的注册选项。 我们的每个服务都可以通过 HTTP 访问。

通过执行这些步骤，您将最终达到微服务级别，其中每个服务将为单个业务功能提供服务。

# 模式设计和数据库选择

任何应用程序的主要部分都是它的数据库选择。 在本节中，我们将研究如何为微服务设计数据库，是否保持其独立，是否保持其共享，以及将哪个数据库转到 sql 还是 NoSQL? 我们将研究如何根据数据类型和业务功能对数据存储进行分类。 有很多选择。 微服务支持多语言持久性。 基于业务功能和需求选择特定数据存储的方法称为多语言持久性。 以下几点根据用例讨论参考哪个数据库:

*   我们可以利用 Apache Cassandra 来支持表格数据，比如库存数据。 它具有分布式一致性和轻量级事务等选项来支持 ACID 事务。
*   我们可以利用 Redis 来支持缓存数据，其中数据模型只是一个键值对。 Redis 的读操作非常快。
*   我们可以利用 MongoDB 来支持以非结构化形式存储的产品数据，并且能够对任何特定的字段进行索引。 面向文档的数据库(如 MongoDB)有强大的选项，比如在特定属性中建立索引以更快地搜索。
*   我们可以利用 GraphQL 来支持复杂的关系。 GraphQL 对于多对多关系非常有用，例如我们的购物车推荐系统。 Facebook 使用 GraphQL。
*   我们可以使用关系数据库来支持遗留系统或需要维护结构化关系数据的系统。 我们在数据不经常更改的地方使用关系数据。

在本节中，我们将详细研究这些要点，并理解数据层应该如何在微服务中。 然后，我们将研究数据库的类型，并了解它们的优点、缺点和用例。 让我们开始吧。

# 如何隔离微服务之间的数据

微服务最难的是我们的数据。 每个微服务都应该通过拥有自己的数据库来维护数据。 数据不能通过数据库共享。 这条规则帮助我们消除了导致不同微服务之间紧密耦合的常见情况。 如果两个微服务共享相同的数据库层，并且第二个服务不知道第一个服务更改了数据库模式，那么它将失败。 因此，服务所有者需要不断保持联系，这与我们的微服务路径不同。

我们可能会想到的一些问题是，数据库将如何留在微服务世界中? 这些服务会共享数据库吗? 如果是，那么共享数据的影响是什么? 让我们来回答这些问题。 我们都知道这句话，【t】拥有就意味着责任【t】。 类似地，如果一个服务拥有一个数据存储，那么它单独负责使其保持最新。 此外，为了获得最佳性能，微服务需要的数据应该位于附近或本地，最好位于微服务容器本身，因为微服务需要经常与之交互。 到目前为止，我们已经了解了如何隔离数据的两个原则:

*   应该对数据进行划分，以便每个微服务(实现某种业务功能)能够轻松地确保数据库是最新的，并且不允许任何其他服务直接访问。
*   与微服务相关的数据应该在附近。 让它远离数据库会增加数据库成本和网络成本。

数据分离的一般过程之一是建立一个包含实体、对象和聚合的领域模型。 假设我们有以下用例—允许客户搜索产品，允许客户购买特定类型的产品，以及允许客户购买产品。 我们有三个功能——搜索、购买和库存。 每个功能都有自己的需求，因此产品数据库存储在产品目录服务中，库存以不同的方式存储，搜索服务查询产品目录服务，并缓存这些结果。

在本节中，我们将通过一个示例详细研究这些规则，该示例将帮助我们决定将数据层保存在何处，以及如何划分数据层以获得最大的优势。

# 假设 1 -数据所有权应该通过业务能力进行管理

决定数据在微服务系统中的位置的主要思想之一是根据业务功能来决定。 微服务只是一种实现业务功能的服务，没有数据存储是不可能实现的。 业务功能定义了微服务的包含区域。 所有属于处理该功能的东西都应该驻留在微服务中。 例如，只有一个微服务应该有客户的个人信息，包括送货地址和电子邮件地址。 另一个微服务可以有客户的购买历史，第三个微服务可以有客户偏好。 负责业务功能的微服务负责存储数据并使其保持最新。

# 假设 2 -为了速度和健壮性复制数据库

在选择在微服务系统中存储数据的位置时，第二个因素是根据范围或位置决定的。 无论数据存储在微服务附近还是远处，都会有很大的变化，即使我们讨论的是相同的数据。 一个微服务可以查询自己的数据库中的数据，或者一个微服务可以查询另一个微服务中的相同数据。 当然，后者也有缺点和紧密的依赖性。 查看当地社区比查看不同城市要快得多。 一旦你决定了数据的范围，你就会意识到微服务需要经常相互交谈。

这种微服务通常会产生一种非常紧密的依赖性，这意味着我们总是固守着同样的旧的单块内容。 为了缓解这种情况，耦合缓存数据库或维护缓存存储通常很方便。 您可以按原样缓存响应，也可以添加读取模型以在特定时间间隔后过期缓存。 拥有本地数据的微服务应该处于最佳位置，根据业务功能来决定特定代码段何时失效。 HTTP 缓存头应该用来控制缓存。 管理缓存就是简单地控制缓存控制头。 例如，行`cache-control: private, max-age:3600`将响应缓存 3600 秒。

在下一节中，我们将探讨如何根据以下标准选择最佳数据库:

*   我的数据如何? 它是一堆表、一个文档、一个键值对还是一个图?
*   我的数据写入和读取频率是多少? 我的写作请求是随机出现的还是在时间上均匀分布的? 是否存在一次读取所有内容的场景?
*   写操作多了还是读操作多了?

# 如何为你的微服务选择数据存储

设计微服务时，最基本的问题之一是*如何选择正确的数据存储?* ,我们将更详细地讨论这个*服务**状态*在[第七章](07.html),【显示】服务状态和军种间的沟通,但在这里,让我们得到基本正确。

选择理想的数据存储的第一步，也是最重要的一步，是找出我们微服务数据的本质。 根据数据的性质，我们可以简要定义以下类别:

*   **短暂数据**:缓存服务器是短暂数据的典型例子。 它是一个临时存储，其目的是通过实时服务信息来增强用户体验，从而避免频繁的数据库调用。 在大多数操作都是读密集型的情况下，这一点尤其重要。 此外，该存储没有额外的持久性或安全性问题，因为它没有数据的主副本。 然而，话虽如此，这也不应该被轻视，因为它必须是高度可用的。 失败会导致糟糕的用户体验，并导致主数据库崩溃，因为它无法处理如此频繁的调用。 这类数据存储的例子包括 Redis、Elasticsearch 等。
*   :日志和消息等数据通常以大容量和大频率出现。 摄取服务在将信息传递到适当的目的地之前处理该信息。 这样的数据存储需要高频写入。 时间序列数据或 JSON 格式等特性也具有优势。 对瞬态数据的支持要求更高，因为它主要用于基于事件的通信。
*   **操作性或功能性数据**:操作性数据关注从用户会话收集的任何信息，例如用户配置文件、用户购物车、愿望列表等等。 作为主要的数据存储，这种微服务提供了更好的用户体验和实时反馈。 为了业务连续性，必须保留这类数据。 在这里，持久性、一致性和可用性要求非常高。 我们可以根据需要使用任何类型的数据存储，提供以下任何结构—json、图、键-值、关系，等等。
*   **事务数据**:从一系列流程或事务中收集的数据，如支付处理、订单管理，必须存储在支持 ACID 控件的数据库中，以避免灾难(我们将主要使用关系数据库来处理事务数据)。 在写这本书的时候，支持事务性数据的 MongoDB 4.0 仍然不可用。 一旦普遍可用，NoSQL 数据存储甚至可以用于事务管理。

# 产品微服务设计

根据我们的需求，我们可以将数据分为以下各个部分:

| **微服务** | **存储类型** |
| 缓存 | 短暂的(例如:麋鹿) |
| 用户评论，评级，反馈，和产品畅销 | 瞬态 |
| 产品目录 | 操作 |
| 产品搜索引擎 | 操作 |
| 订单处理 | 事务 |
| 订单执行 | 事务 |

对于我们的产品目录数据库，我们将进行以下设计。

在本章中，我们将讨论产品目录服务，它要求我们使用操作数据存储。 我们将使用 MongoDB。 一个产品至少有以下几项:变量、价格、层次结构、供应商、反馈电子邮件、配置、描述，等等。 我们将使用以下模式设计，而不是在一个文档中获取所有内容:

```js
{"desc":[{"lang":"en","val":"TypescriptMicroservicesByParthGhiya."}],"name":"TypescriptMicroservices","category":"Microservices","brand":"PACKT","shipping":{"dimensions":{"height":"13.0","length":"1.8","width":"26.8"},"weight":"1.75"},"attrs":[{"name":"microservices","value":"exampleorientedbook"},{"name":"Author","value":"ParthGhiya"},{"name":"language","value":"Node.js"},{"name":"month","value":"April"}],"feedbackEmail":"ghiya.parth@gmail.com","ownerId":"parthghiya","description":"thisistestdescription"}
```

这种模式设计的一些优点如下:

*   很容易进行分面搜索，以毫秒的速度返回结果
*   每个索引都以`_id`结尾，这对分页很有用
*   可以对各种属性进行高效排序

# Microservice 会展方面

在这一节中，我们将着眼于一些贯穿全书的共同发展方面。 我们将了解一些常见的方面，例如使用哪些 HTTP 消息代码、如何设置日志记录、保留哪些类型的日志记录、如何使用 PM2 选项以及如何跟踪请求或将唯一标识符附加到微服务。 让我们开始吧。

# HTTP 代码

**HTTP 代码**主导着标准 API 通信，是任何通用 API 的通用标准之一。 它解决了向服务器发出的任何请求的常见问题，是否成功，是否产生了服务器错误，等等。 HTTP 用 HTTP 代码解析每个请求，这些代码具有指示代码性质的范围。 HTTP 代码是基于各种代码的标准([http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html))，并相应地采取响应操作，因此这里适用的是不重复发明轮子的概念。 在本节中，我们将研究一些标准代码范围及其含义。

# 1 xx -信息

**1xx**代码提供了基本功能，例如在后台发生的操作、切换协议或初始请求的状态。 例如,`100 Continue`表明服务器收到请求头,现在等待请求主体,`101 Switching Protocols`表明客户端从服务器请求一个协议改变和请求已被批准,和`102`表明操作在后台发生的,需要时间来完成。

# 2 xx -成功

这表明使用 HTTP 请求中使用的信息成功代码已经取得了一定程度的成功。 它将几个响应打包到特定的代码中。 例如，`200 Ok`表示没有出错，GET 或 POST 请求成功。 `201 Created`表示已经完成了 GET 或 POST 请求，并为客户端创建了新的资源。 `202 Accepted`表示请求已被接受并正在处理中。 `204 No Content`意味着没有内容从服务器返回(非常类似于`200`)。 `206 Partial Content`通常用于分页响应，表示有更多数据要跟进。

# 3xx – redirections

**3xx**范围都是关于资源或端点的状态。 它指示在服务器仍在接受通信时必须采取哪些额外的操作来完成该请求，但所联系的端点不是系统中的正确入口点。 最常见的代码使用`301 Moved Permanently`,这意味着未来的请求必须由不同的 uri,`302 Found`,这表明需要一个临时重定向出于某种原因,`303 See other`,它告诉浏览器看到另一个页面,和`308 Permanent Redirect`,这表明该资源的永久重定向(这是`301`一样, 但不允许 HTTP 方法更改)。

# 4xx -客户端错误

由于传统的`404 Not found`错误，这一段代码最为人所知，这是一个众所周知的 url 占位符，用来表示格式不正确的 url。 这段代码表明请求有问题。 其他众所周知的代码包括`400 Bad Request`(语法不正确的请求)、`401 Unauthorized`(缺少来自客户机的身份验证)和`403 Forbidden`(用户没有特权)。 另一个常见代码是`429 Too Many Requests`，它用于限速请求，以指示来自特定客户机的流量被拒绝。

# 5xx -服务器错误

这段代码表明服务器上出现了处理错误或服务器中出现了错误。 每当发出**5xx**代码时，它表示服务器中存在某种客户端无法修复的问题，必须进行相应的处理。 一些广泛使用的代码是`500 Internal Server Error`(这表示在服务器的软件中发生了错误，没有透露任何信息)、`501 Not Implemented`(这表示尚未实现端点，但仍在请求)、 和`503 Service Unavailable`(这表示服务器由于某种原因宕机，不能再处理任何请求)。 在接收到`503`时，必须采取适当的操作重新启动服务器。

# 为什么 HTTP 代码在微服务中至关重要?

微服务是完全分布式的，而且是不断移动的。 因此，没有任何标准的通信手段，我们将无法触发相应的故障安全措施。 例如，如果我们实现断路器模式，电路应该知道每当它接收到**5xx**系列代码时，它应该保持电路处于开路状态，因为服务器不可用。 类似地，如果它接收到`429`，那么它应该阻止来自特定客户端的请求。 完整的微服务生态系统包括代理、缓存、rpc 和其他以 HTTP 为公共语言的服务。 根据上述代码，他们可以采取相应的操作。

在下一节中，我们将学习日志记录方面的内容以及如何在微服务中处理日志记录。

# 通过日志审计

到目前为止，我们听说微服务是分布式的，服务是不断变化的。 我们需要跟踪所有服务及其抛出的输出。 使用`console.log()`是一个非常糟糕的实践，因为我们无法跟踪所有的服务，因为`console.log()`没有固定的格式。 此外，每当出现错误时，我们都需要一个堆栈跟踪来调试可能的问题。 要实现分布式日志记录，我们将使用`winston`模块([https://github.com/winstonjs/winston](https://github.com/winstonjs/winston))。 它有各种选项，比如日志级别、日志格式等等。 对于每个微服务，我们将传递一个唯一的微服务 ID，当我们聚合日志时，该 ID 将标识它。 对于聚合，我们将使用著名的 ELK 堆栈，在[第 9 章](09.html)、*部署、日志记录和监控*中进行了描述。 下面是常用的各种日志，按优先级顺序排序:

*   **Fatal/emergency(0)**:这是最致命的级别，当系统无法恢复或正常工作时使用。 这会导致诸如关闭或其他令人发指的错误。
*   **Alert(1)**:当收到此严重日志时，必须立即采取措施防止系统关闭。 这里的关键区别是系统仍然可用。
*   **Critical(2):在这里，不需要立即采取行动。 这个级别包括无法连接到套接字、无法获得最新的聊天消息等情况。**
*   **Error(3)**:This is a problem that should be investigate .**Error(3) “系统管理员”必须得到通知，但我们不需要把他从床上拽起来，因为这不是紧急情况。 它通常用于跟踪整体质量。**
*   **Warning(4)**:当可能存在错误或可能没有错误时使用。 警告条件接近于错误，但它们不是错误。 它们表示可能导致错误的潜在有害情况或事件。
*   **Notice(5)**:该级别是普通日志，但有一些重要的条件。 例如，您可能会收到消息，例如 Caught SIGBUS 试图在....中转储核心
*   **Info(6)**:此级别用于不引人注意的信息，例如服务器已经运行*x*小时和有趣的运行时事件。 这些日志在控制台中立即可见，因为这些日志的目的是保守的。 这些日志应该保持在最低限度。
*   **Debug(7)**:用于系统流程的详细信息。 它包括用于调试的消息，例如，类似于 open file… 或为 productId 47 获取产品。

Logs need to be enabled. If you enable fatal logs, then all logs will be seen. If you enable info logs, then only info and debug logs will be seen. Logs for all levels have their custom method in Winston and we can add our own format.

# PM2 进程管理器

Node.js 是单线程的，这意味着任何使用 JavaScript`throw`语句都会引发一个异常，该异常必须使用`try...catch`语句处理。 否则，Node.js 进程将立即退出，使其无法处理任何进一步的请求。 由于 Node.js 运行在单个进程未捕获的异常上，因此需要谨慎处理。 否则，它将崩溃并导致整个应用程序崩溃。 因此，Node.js 中的黄金法则是*如果任何异常冒泡到顶部而没有被处理，我们的应用程序就会死亡*。

**PM2**是一个流程管理器，旨在保持我们的服务永远存在。 它是一个带有内置负载平衡器的生产流程管理器，是微服务的完美候选。 PM2 非常方便，因为它允许我们用简单的 JSON 格式声明每个微服务的行为。 PM2 是一种高级任务运行程序，具有内置的监视和零停机实用程序。 扩展 PM2 命令只是输入我们想要增加或减少的实例数量的问题。 使用 PM2 启动一个新进程将启动该进程的 fork 模式，并让负载均衡器处理其余的工作。 PM2 充当主进程和进程工作者之间的轮询，这样我们就可以同时处理额外的负载。 PM2 提供的一些标准部署特性如下:

| `pm2 start <process_name>` | 以 fork 模式启动进程，并在服务器宕机时自动重启进程 |
| `pm2 stop <process_name>` | 停止 PM2 进程 |
| `pm2 restart <process_name>` | 用更新后的代码重新启动进程 |
| `pm2 reload <process_name>` | 重新加载 PM2 进程，没有停机时间 |
| `pm2 start <process_name> -i max` | 在最大数量的 fork 模式下启动 PM2 进程; 也就是说，它将根据可用 cpu 的数量生成最大实例数 |
| `pm2 monit` | 监控 PM2 进程 |
| `pm2 start ecosystem.config.js --env staging` | 启动一个进程，从`ecosystem.config.js`获取配置 |

PM2 还可以用作部署工具或 CI/CD 的高级手段。 你只需要在`ecosystem.config.js`文件中定义你的部署脚本，如下所示:

```js
"deploy": {
    "production": {
        "key": "/path/to/key.pem", // path to the private key to authenticate
        "user": "<server-user>", // user used to authenticate, if its AWS than ec2-user
        "host": "<server-ip>", // where to connect
        "ref": "origin/master",
        "repo": "<git-repo-link>",
        "path": "<place-where-to-check-out>",
        "post-deploy": "pm2 startOrRestart ecosystem.config.js --env production"
    },
}
```

然后，我们要做的就是点击下面的命令:

```js
pm2 deploy ecosystem.config.js production
```

该命令充当本地部署工具。 添加路径、PEM 文件密钥等内容是我们可以连接到服务器的步骤。 一旦使用指定用户连接到服务器，PM2 进程就会启动，我们就可以运行应用程序了。 最新的 Git 存储库将被克隆，然后 PM2 将在 forever 选项中启动`dist/Index.js`文件。

# 跟踪请求

**跟踪请求**来源是非常重要的，因为有时我们需要在我们的系统中重建客户的整个旅程。 它提供了关于系统的有用信息，例如延迟的来源。 它还使开发人员能够通过使用某些惟一的微服务 ID 搜索所有聚合日志来观察单个请求是如何被处理的，或者通过传递一个时间框架来发现用户的总体旅程。 以下是通过 Winston 生成的日志示例:

```js
{ level: 'info', serviceId: 'hello world microservice' , 
  message: 'What time is the testing at?', 
  label: 'right meow!', timestamp: '2017-09-30T03:57:26.875Z' }
```

所有重要的数据都可以从日志中看到。 我们将使用 ELK 堆栈作为我们的日志。 麋鹿拥有巨大的优势,因为它结合了以下三个工具的力量——**Logstash**(配置为读取日志或注册事件从各种各样的来源和日志事件发送到多个源),**Kibana**(一个可配置的 web 指示板,用于查询 Elasticsearch 日志信息并将其呈现给用户), Elasticsearch(一个基于 Lucene 的搜索服务器，用于收集日志、解析日志、存储日志以供以后使用，提供 RESTful 服务和无模式 JSON 文档)。 它有以下优点:

*   **Winston**的每个实例都配置了 ELK。 因此，我们的日志服务是外部化的，日志的存储是集中的。 因此，存在一个可以跟踪请求的单一数据源。
*   由于 Winston 的自动模式定义和正确的格式，我们拥有日志结构的数据。 例如，如果我想查询从`4:40`到`4.43`的所有日志，我只需要使用 Elasticsearch 查询，因为我知道我的所有日志都有一个固定的 JSON 级别的时间组件。
*   Winston 日志格式负责创建和传递所有请求的相关标识符。 因此，如果需要，可以通过查询特定参数轻松跟踪特定于服务器的日志。
*   我们的日志可以通过 Elasticsearch 搜索。 Elasticsearch 提供 Kibana 和 REST api，可以调用这些 api 来查看数据源中所有数据的任何时间点。 基于 lucene 的实现有助于更快地获取结果。
*   日志级别可以在 Winston 中动态更改。 我们可以有各种日志级别，并且根据日志的优先级，较低级别的日志可能会被看到，也可能看不到。 这对于解决生产级别的问题非常有帮助。

在本节中，我们查看了日志记录以及它如何解决理解客户行为等问题(客户在页面上花费了多少时间，每个页面上的一个操作花费了多少时间，可能出现的问题有哪些，等等)。 在下一节中，我们将开始开发购物车微服务。

# 为购物车开发一些微服务

在本节中，我们将为购物车开发一些微服务，这些微服务由其业务功能惟一标识。 所以，在动手之前，让我们快速地了解一下当前的问题。 这个单一的购物车运行得很好，但随着数字化的到来，交易量出现了巨大的增长——比最初的估计增加了 300-500 倍。 对端到端架构进行了回顾，它有以下局限性，在此基础上介绍了微服务架构:

*   **坚固性和坚固性**:由于错误和卡住的线程，系统的坚固性受到很大的影响，这迫使 Node.js 应用服务器不接受任何新的事务，并执行强制重启。 内存分配问题和数据库锁线程是主要问题。 某些资源密集型操作会影响整个应用程序，资源分配池总是会被消耗。
*   **部署中断**:由于添加越来越多的功能，服务器中断窗口的增加主要是由于服务器启动时间。 大的`node_modules`是罪魁祸首。 由于整个应用程序被打包为一个整体，所以整个应用程序需要一次又一次地安装`node`模块，然后启动我们的节点- http 服务器。
*   **锐度**:代码的复杂度随着时间的推移呈指数增长，工作的分布也是如此。 团队之间创建了紧密耦合依赖关系。 结果，更改更难实现和部署。 影响分析变得太复杂而无法执行。 结果，就像*修复一个 bug，其余 13 个出现*。 当`node_modules`大小超过 1 GB 时，这种复杂性就会上升。 这些并发症最终导致**持续整合**(**CI**)和单元测试套管停止。 最终，产品的质量恶化了。

这种情况和问题需要一种循序渐进的方法。 这种情况要求采用微服务开发方法。 在本节中，我们将研究微服务设置方法，它将为我们提供各种优势，如选择性服务扩展、技术独立性(容易迁移到新技术)、包含故障，等等。

# 行程

让我们快速浏览一下我们在这个练习中将要执行的行程:

*   **开发设置和前提模块**:在本节中，我们将总结在项目中使用的开发工具和`npm`模块。 我们将研究应用程序属性、自定义中间件、依赖注入等先决条件。
*   **应用程序目录配置**:我们将分析我们将在其他微服务中使用的结构，并了解我们将需要的所有文件以及将逻辑写入何处。
*   **配置文件**:我们将查看所有可以指定各种设置的配置文件，例如数据库主机名、端口 URL 等等。
*   **处理数据**:我们将简要总结代码模式，以及它们如何支持最佳的开发人员输出，并使开发人员的生活更轻松。
*   :我们将分析`package.json`和 Docker 文件，看看我们如何使用这两个文件来让我们的微服务准备好服务任何服务请求。

那么，让我们开始我们的旅程吧。

# 开发设置和先决条件模块

在本节中，我们将看看在开发和创建我们的**开发沙箱时需要注意的几个方面。** 我们将得到我们将使用的所有节点模块的概述，以及每个`node`模块将满足的核心方面。 所以，是时候动手了。

Note: We saw how to write custom types in [Chapter 2](02.html), *Gearing up for the Journey*, for any node module that is not written in ES6\. We will leverage this for any module whose types are not available in the `DefinitelyTyped` repository.

# 库模式

在本节中，我们将了解存储库模式，它使我们能够将代码放在一个地方。 TypeScript 引入了泛型(就像 Java 中的特性一样)，我们将在微服务中充分利用它。 存储库模式是用于创建企业级应用程序的最广泛的模式之一。 通过创建一个用于数据库操作和业务逻辑的新层，它使我们能够直接使用应用程序中的数据。

将泛型和存储库模式结合起来可以带来无数的好处。 使用 JavaScript 应用程序时，我们需要处理应用程序之间的代码共享和模块化等问题。 泛型存储库模式解决了这个问题，当我们有一个泛型抽象类(或者很多泛型，取决于业务能力)时，泛型存储库模式为我们提供了编写数据抽象的能力，并且独立于数据模型重用实现层，只将类型传递给某人的类。 当我们讨论存储库模式时，它是一种存储库，我们可以将数据库(CRUD)的所有操作保存在一个位置，用于任何通用业务实体。 当需要在数据库中执行操作时，应用程序调用存储库方法，从而为调用者提供透明性。 将其与泛型结合起来可以得到一个抽象，一个具有所有公共方法的基类。 我们的`EntityRepository`只扩展了基类，包含了数据库操作的所有实现。

This pattern follows the open/closed principle, where the base class is open for extension but closed for modification.

它有各种优点，如下:

*   当所有其他实体都应该具有类似的操作时，您只需要为所有常见操作(如 crud)编写一个类，就可以将它用作一种可扩展性度量
*   可以在不涉及数据访问逻辑的情况下对业务逻辑进行单元测试
*   数据库层可以重用
*   数据库访问代码是集中管理的，以便实现任何数据库访问策略，就像缓存一样，这是在公园里散步

# 配置应用程序属性

按照 twelve-factor 标准(回想一下,*twelve-factor microservices*应用,部分[第一章](01.html),*揭穿 microservices*),一个代码库应满足多个环境,如 QA,开发,生产,等等。 确保我们的应用程序中有应用程序属性文件，我们可以在其中指定环境名称和与环境相关的东西。 Config([https://www.npmjs.com/package/config](https://www.npmjs.com/package/config))就是这样一个模块，它可以帮助您组织所有配置。 该模块只读取`./config`目录中的配置文件(它应该与`package.json`处于同一级别)。

配置的显著特点如下:

*   它可以支持 YAML, YML, JSON, CSV, XML 等格式。
*   它可以创建一个与`package.json`并行的目录配置，并在其中创建一个文件`default.ext`(这里，`.ext`可以是前面提到的任何一种格式)。
*   要读取配置文件，只需使用以下代码行:

```js
import * as config from 'config';
const port = config.get('express.port');
```

*   它支持各种配置文件，其中维护一个层次结构以支持各种环境。
*   它甚至支持多个节点实例; 非常适合微服务。

# 自定义健康模块

有时，向应用程序添加新模块会导致应用程序出现故障。 我们需要自定义健康模块实际上保持关注服务并提醒我们,服务的顺序(服务发现这个,我们将看看在第六章,*服务注册和发现*)。 我们将使用`express-ping`([https://www.npmjs.com/package/express-ping](https://www.npmjs.com/package/express-ping))来找出节点的健康状况。 通过在我们的中间件中引入这个模块，我们可以公开一个简单的 API，该 API 将向操作人员和其他应用程序告知它的内部运行状况。

`express-ping`的显著特点如下:

*   它是一个零配置模块，只需将其注入中间件，就会公开一个运行状况端点。
*   要使用此模块，只需使用以下代码行:

```js
import * as health from 'express-ping';
this.app.use(health.ping());
```

*   仅添加前面的 LOCs 将暴露一个`<url>/health`端点，我们可以将其用于健康检查目的。 我们可以为我们公开的`/ping`API 添加授权访问，甚至使用中间件，这只是简单的旧表达:

```js
app.get('/ping', basicAuth('username', 'password'));
app.use(health.ping('/ping'));
```

*   这个端点可以用于任何地方，只用于检查应用程序的健康状况。

# 依赖注入和控制反转

在本节中，我们将看到如何使用依赖注入和控制反转等基本原则。 由于有 Java 背景，我倾向于在任何应用程序中使用这些原则，以便使我的开发过程更顺畅。 幸运的是，我们有完全匹配我们需求的模块。 我们将使用`inversify`([https://www.npmjs.com/package/inversify](https://www.npmjs.com/package/inversify))作为控制容器的反转，使用`typedi`([https://www.npmjs.com/package/typedi](https://www.npmjs.com/package/typedi))进行依赖注入。

# Inversify

**控制倒置**(**IOC**)是指获得自由、更灵活、更少依赖他人。 假设你使用的是台式电脑，你是被奴役的(或者说被控制的)。 你必须坐在屏幕前看着它，用键盘打字，用鼠标导航。 编写得很糟糕的软件同样可以奴役你。 如果你把台式机换成笔记本电脑，那么你就有了反转控制。 你可以很容易地拿着它四处走动。 所以，现在你可以用电脑控制你所处的位置，而不是由电脑控制它。 软件中的 IOC 非常类似。 传统上，IOC 是一种设计原则，在这种设计原则中，计算机程序的定制编写部分从通用框架接收控制流。 我们有`inversifyJS`作为`npm`模块。 根据他们的官方文件:

<q>*InversifyJS is a lightweight inversion of control container for TypeScript and JavaScript applications. An IOC container will use a class constructor to identify and inject its dependencies. It has a friendly API and encourages the usage of best OOP and IoC practices adhering to SOLID principles.*</q>

# Typedi

依赖注入是类、组件和服务指定它们依赖哪些库的一种方式。 通过简单地将依赖项注入到微服务中，服务就能够直接引用依赖项，而不是在服务注册中心或使用服务定位器查找依赖项。 封装任何服务、发现服务和分配负载的能力是微服务的一个极有价值的补充。 Typedi 是 JavaScript 和 TypeScript 的依赖注入工具。 使用 Typedi 非常简单。 您所要做的就是创建一个容器，并开始在该容器上使用依赖注入原则。 Typedi 提供了各种注释，如`@Service`、`@Inject`等。 您甚至可以创建自己的自定义装饰器。

# TypeORM

受 hibernate 和 doctrine 等框架的启发，实体框架**TypeORM**([https://www.npmjs.com/package/typeorm](https://www.npmjs.com/package/typeorm))是一个支持活动记录和数据映射模式的 ORM 框架，与所有其他 JavaScript ORM 不同。 这使我们能够以最高效的方式编写高质量、松耦合、可伸缩和可维护的应用程序。 它有以下优点:

*   使用多个数据库连接
*   适用于多种数据库类型
*   查询缓存
*   钩子，例如订阅者和侦听器
*   写在打印稿
*   同时支持数据映射器和活动记录模式
*   复制
*   连接池
*   流化原始结果(响应式编程)
*   渴望与懒惰的关系
*   支持 SQL 和 NoSQL 数据库

# 应用程序目录配置

该应用程序的目录结构主要关注基于关注点分离的架构方法。 每个文件夹结构都有特定于文件夹名称的文件。 在下面的截图中，可以看到整体结构和详细结构:

![](assets/2795b24c-b3ae-493a-811d-b583cab39c49.png)

Configuration structure

在前面的屏幕截图中，您可以看到两个文件夹结构。 第一个是高亮显示重要文件夹的高级和整体文件夹结构，而第二个是`src`文件夹的详细展开视图。 文件夹结构遵循*关注点分离*方法，以消除代码重复并在控制器之间共享单例服务。

In computer science, **separation of concerns** (**SoC**) is a design principle for dividing a computer program into distinct sections or capabilities so that each section addresses a separate concern and is independent of the other. A concern is a set of information that affects the code of any application.

让我们了解文件夹结构和它包含的文件，以及文件夹实际处理的问题。

# src - data-layer

这个文件夹负责数据的整体组织、存储和可访问性方法。 模型定义和铱文件可以在这里找到。 它有以下文件夹:

*   **适配器**:实现 MongoDB 连接方法的设置，用于连接到 MongoDB 数据库，并在连接、错误、打开、断开、重新连接和强制退出方法上添加事件
*   **数据抽象**:它既有表示每个 MongoDB 集合结构的模式，也有表示集合中每个数据集的文档
*   **data -agents**:它具有针对每个 MongoDB 集合的数据存储的查询事务
*   **模型**:它有一个 TypeScript 类来表示 MongoDB 文档中描述的数据

# src /层

这个文件夹有业务逻辑的实现和服务层或中间件层需要的其他资源，如下所示:

*   **安全性**:如果我们想要特定微服务级别上的一些安全性或令牌，这就是我们添加身份验证逻辑的地方(通常，我们不会在单个服务级别上编写身份验证层)。 相反，我们在 API 网关级别上编写它，我们将在[第五章](05.html)，*理解 API 网关*中看到。 在这里，我们将为服务注册/注销、验证、内部安全、与服务注册中心通信的微服务、API 网关等等编写代码。
*   **验证器**:这将具有模式和处理逻辑，用于验证 API 请求发送的数据。 我们将在这里编写类验证器([https://www.npmjs.com/package/class-validator](https://www.npmjs.com/package/class-validator))模式，以及一些自定义验证函数。

# src /服务层

这个文件夹包括以路由的形式建立 API 端点的过程，路由将处理所有对数据请求的响应。 它有以下文件夹:

*   `controllers`:作为处理任何与路由相关的数据请求的基础。 自定义`controllers``npm`模块特点是`routing-controllers`(【显示】https://www.npmjs.com/package/routing-controllers)使用内置的修饰符,如`@Get`、`@Put`,`@Delete`,`@Param`等等。 这些函数实现了基本的 GET、POST、DELETE 和 PUT 方法，用于通过 RESTful API 与数据库进行事务处理。 我们甚至可以有套接字初始化代码等等。 我们将使用依赖注入来注入这里将要使用的一些服务。
*   `request`:它有 TypeScript 接口来定义和显示控制器中每个不同类型请求的属性。
*   `response`:它有 TypeScript 接口来定义和显示控制器中每一种不同类型响应的属性。

# src/middleware

它包含具有任何服务器配置的资源，以及存储可以跨任何应用程序共享的任何实用程序进程的特定位置。 我们可以有集中配置，如`aslogger`，`cache`，`elk`，等等:

*   `common`:它具有记录器模块的实例化，可以在整个应用程序中共享。 本模块基于`winston`([https://www.npmjs.com/package/winston](https://www.npmjs.com/package/winston))。
*   `config`:这有特定于供应商的实现。 我们将拥有这里定义的 express 配置和 express 中间件，以及组织 REST API 端点的所有重要配置。
*   `custom-middleware`:这个文件夹将有我们所有定制的中间件，我们可以在任何控制器类或任何特定的方法中使用。

在下一节中，我们将查看一些配置和定义应用程序的配置文件，并确定它将如何运行。 例如，它将运行的端口、数据库连接的端口、安装的模块、传输配置，等等。

# 配置文件

让我们来看看一些我们将在整个项目中使用的配置文件，并使用它们在不同的环境或按照用例管理项目:

*   **`default.json`**:Node.js 有一个优秀的模块`node-config`。 您可以在`config`文件夹中找到与`package.json`平行的`config`文件。 在这里，您可以根据环境选择多个配置文件。 例如，将首先加载`default.json`，然后是`{deployment}.json`，以此类推。 下面是一个示例文件:

```js
{
    "express": {
        "port": 8081,
        "debug": 5858,
        "host": "products-service"
    }
 }
}
```

*   **`src/Index.ts`**:**通过创建`middleware`/`config`/`application`中定义的应用程序的新对象来初始化我们的应用程序。 它导入了用于初始化依赖注入容器的反映元数据。**
***   **`package.json`**:作为所有 Node.js 应用程序中的 manifest 文件。 它在两个部分中描述了构建应用程序所需的外部库`dependencies`和`devDependencies`。 这提供了一个具有构建、运行和打包模块的外部命令的`scripts`标记。*   `tsconfig.json`:当 TypeScript 执行编译到 JavaScript 的任务时，它提供了一些选项。 例如，如果我们有`sourceMaps:true`，我们就可以通过生成的 sourcemaps 来调试 TypeScript 代码。*   `src/data-layer/adapters/MongoAccess.ts`:它将连接到 MongoDB 数据库，并将各种事件处理程序附加到 MongoDB 的各种事件，如`open`、`connected`、`error`、`disconnected`、`reconnected`，等等:**

```js
export class MongooseAccess {
  static mongooseInstance: any;
  static mongooseConnection: Mongoose.Connection;
  constructor() {
    MongooseAccess.connect();
  }
  static connect(): Mongoose.Connection {
    if (this.mongooseInstance) {
      return this.mongooseInstance;
    }
    let connectionString = config.get('mongo.urlClient').toString();
    this.mongooseConnection = Mongoose.connection;
    this.mongooseConnection.once('open', () => {
      logger.info('Connect to an mongodb is opened.');
    });
    //other events
  }
```

*   `src/middleware/config/Express.ts`:**这是我们的 express 中间件所在的位置。 我们将附加标准配置，如`helmet`、`bodyparser`、`cookieparser`、`cors origin`等，并建立我们的`controllers`文件夹，如下所示:**

```js
setUpControllers(){
  const controllersPath = 
       path.resolve('dist', 'service-layer/controllers');
  useContainer(Container);
  useExpressServer(this.app,
    {
      controllers: [controllersPath + "/*.js"],
      cors: true
    }
  );
}
```

# 处理数据

与大多数接受并处理来自客户端的请求的 web 服务器一样，我们这里也有非常类似的事情。 我们只是在宏观层面上将事物粒度化。 整个流程如下图所示:

![](assets/3cece010-b02d-4e55-916e-51149307de81.png)

Processing data

让我们通过前面图中的每个部分选取任意示例端点来了解这个过程。 你可以在`chapter-4/products-catalog service`中找到整个样本:

1.  一个基于产品属性放置特定产品的 API 请求被发送到服务器`http://localhost:8081/products/add-update-product`:

```js
body: {//various product attributes}
```

2.  路径为`/products`的注册控制器根据`URI /products/`捕获请求。 如果一个中间件在`Express.ts`注册，它将首先被触发; 否则，将调用控制器方法。 注册中间件很简单。 用以下代码创建一个中间件类:

```js
import { ExpressMiddlewareInterface } from "routing-controllers";
export class MyMiddleware implements ExpressMiddlewareInterface {

  use(request: any, response: any, next?: (err?: any) => any): any {
    console.log("custom middleware gets called, here we can do anything.");
    next();
  }
}
```

3.  要在任何控制器中使用此中间件，只需在任何方法/控制器上使用`@UseBefore`和`@UseAfter`装饰器。
4.  因为我们想要执行一些核心逻辑(例如从缓存中选择响应或日志记录)，所以`middleware`函数首先被执行。 这存在于`middleware/custom-middleware/MyMiddleWare.ts`中。 使用 Node.js 的`async`功能，该方法将完成必要的工作，然后使用`next()`继续下一个请求。
5.  在自定义中间件中，我们可以进行各种检查; 例如，我们可能希望只在存在有效的`ownerId`时公开 api，或者只向授权的销售者公开 api。 如果请求没有有效的`ownerId`，则请求将不再继续执行应用程序的其余部分，我们可以抛出一个错误，表明真实性或无效的`productId`。 但是，如果`ownerId`有效，则请求将继续通过路由进行。 这就是`MyMiddleWare.ts`的作用。 下一个部分将通过控制器。
6.  下一部分是由路由控制器提供的装饰器定义的`@JsonControllers`。 我们定义了路由控制器和 post API，用于添加和更新产品:

```js
@JsonController('/products')
@UseBefore(MyMiddleware)
export class ProductsController {
  constructor() { }

  @Put('/add-update-product')
  async addUpdateProduct( @Body() request: IProductCreateRequest,
    @Req() req: any, @Res() res: any): Promise<any> {
    //API Logic for adding updating product
  }
}
```

这将为`API <host:url>/products/add-update-product`创建 PUT 请求。 `@Body`注释将把请求体的强制转换转换为变量 request 中的`IProductCreateRequest (src/service-layer/request/IProductRequest.ts`(如`addIpdateProduct`方法的参数所示)，该转换在整个方法中都可用。 `request`和`responses`文件夹包含各种`request`和`response`对象的转换。

7.  控制器的第一部分是验证请求。 验证和安全逻辑将在`src/business-layer`文件夹中。 在`validator`文件夹中，我们将有`ProductValidationSchema.ts`和`ProductValidatorProcessor.ts`。 在`ProductValidationSchema.ts`,添加验证模式规则(不同的验证消息,通过它,我们想要确定一个请求是否正确或者垃圾数据)使用`class-validator`([https://www.npmjs.com/package/class-validator](https://www.npmjs.com/package/class-validator))内置的修饰符,(`@MinLength,``@MaxLength`,【显示】,等等):

```js
export class ProductValidationSchema {
  @Length(5, 50)
  name: string;

  @MinLength(2, { message: "Title is too Short" })

  @MaxLength(500, { message: "Title is too long" })
  description: string;

  @Length(2, 15)
  category: string;

  @IsEmail()
  feedbackEmail: string;
  //add other attributes.
}
```

8.  接下来，我们将使用这些消息来验证请求对象。 在`ProductValidationProcessor.ts`中，创建一个返回合并消息数组的验证器方法:

```js
async function validateProductRequest(productReqObj: any): Promise<any> {
  let validProductData = new ProductValidationSchema(productReqObj);
  let validationResults = await validate(validProductData);
  let constraints = []
  if (validationResults && validationResults.length > 0) {
    forEach(validationResults,
      (item) => {
        constraints.push(pick(item, 'constraints', 'property'));
      });
  }
  return constraints;
}
```

9.  在`ProductsController.ts`中，调用该方法。 如果请求中有错误，请求将停止在那里，不会传播到 API 的其他部分。 如果响应是有效的，那么它将通过数据代理将数据推送到 MongoDB:

```js
let validationErrors: any[] = await validateProductRequest(request);
logger.info("total Validation Errors for product:-", validationErrors.length);
if (validationErrors.length > 0) {
  throw {
    thrown: true,
    status: 401,
    message: 'Incorrect Input',
    data: validationErrors
  }
}
let result = await this.productDataAgent.createNewProduct(request);
```

10.  当请求有效时，控制器`ProductController.ts`调用数据层中的`ProductDataAgent.ts`方法`createNewProduct(..)`，以便将数据放入 MongoDB。 此外，基于 Mongoose 模式定义，它将自动维护一个复制检查条目:

```js
@Put('/add-update-product')
async addUpdateProduct(@Body() request: IProductCreateRequest,
                       @Req() req: any, @Res() res: any): Promise < any > {
  let validationErrors: any[] = await validateProductRequest(request);
  logger.info("total Validation Errors for product:-", validationErrors.length);
  if(validationErrors.length> 0) {
    throw {
      thrown: true,
      status: 401,
      message: 'Incorrect Input',
      data: validationErrors
    }
  }
  let result = await this.productDataAgent.createNewProduct(request);
  if(result.id) {
    let newProduct = new ProductModel(result);
    let newProductResult = Object.assign({ product: newProduct.getClientProductModel() });
    return res.json(<IProductResponse>(newProductResult));
  }else{
    throw result;
  }
}
```

服务层中的控制器不仅通过数据代理(用于协商对数据存储的查询)提供对数据层的访问，而且还提供对业务层的访问入口点，以处理其他业务规则，如验证产品输入。 `ProductDataAgent.ts`方法返回 MongoDB 返回的对象。 它还有其他的方法，如`deleteProduct`、`findAllProducts`、`findProductByCategory`等。

11.  在用`ProductDataAgent.ts`中的数据存储完成事务之后，一个普通对象形式的承诺返回给`ProductController.ts`，表示失败或成功。 当一个成功的产品添加到数据库时，将返回与 MongoDB 的`ObjectID()`一起插入的对象。 与产品相关的数据被构造为`ProductModel`，并将解析为`IProductResponse`至`ProductController.ts`:

```js
async createNewProduct(product: any): Promise < any > {
  let newProduct = <IProductDocument>(product);
  if(newProduct.id) {
    let productObj = await ProductRepo.findOne({ productId: newProduct.id });
    if (productObj && productObj.ownerId != newProduct.ownerId) {
      return { thrown: true, success: false, status: 403, message: "you are not the owner of Product" }
    }
  }
  let addUpdateProduct = await ProductRepo.create(newProduct);
  console.log(addUpdateProduct);
  if(addUpdateProduct.errors) {
    return { thrown: true, success: false, status: 422, message: "db is currently unable to process request" }
  }
  return addUpdateProduct;
}
```

如果在处理`ProductDataAgent.ts`中的查询时发生了一些错误，例如与数据存储的连接中断，将以错误消息的形式返回结果。 如果已经存在具有相同名称的对象，则会抛出类似的错误响应。

这就完成了数据如何流经应用程序的示例。 基于许多后端应用程序和横切因素，这是为了有一个平稳的流程和消除冗余代码而设计的。

同样，本项目还会有其他 api，如下所示:

*   索取所有产品
*   GET 请求通过 ID 获取产品
*   GET 请求按产品类型获取产品
*   删除请求删除单个产品

# 准备好(打包)。 json 和码头工人)

在本节中，我们将看看如何用`package.json`编写脚本，然后使用 Docker 实现整个过程的自动化。

# package.json

既然我们已经知道了数据是如何流动的，那么让我们来了解一下如何让它为服务做好准备。 安装 TypeScript 和`rimraf`作为依赖项，并在`scripts`标签中添加以下内容:

```js
"scripts": {
  "start": "npm run clean && npm run build && node ./dist/index.js",
  "clean": "node ./node_modules/rimraf/bin.js dist",
  "build": "node ./node_modules/typescript/bin/tsc"
},
```

要运行整个进程，请执行如下命令:

```js
npm run start
```

这将首先删除存在的`dist`文件夹，然后，基于`src`文件夹，它将编译该文件夹并生成`dist`文件夹。 一旦生成了`dist`，我们就可以同时使用`node ./dist/Index.js`和`npm run start`运行服务器。

在后面的章节中，我们将在这里做更多的事情，包括测试覆盖率和生成 swagger 文档。 我们的构建脚本应该包括以下内容:

*   通过`swagger-gen`生成文档
*   调用`Express.ts`，它将与中间件和依赖注入一起配置所有路由
*   `tsc`命令将使用`tsconfig.json`中的`"outputDirectory":"./dist"`属性将 TypeScript 文件编译成 JavaScript 文件，以确定 JavaScript 文件应该放在哪里
*   SwaggerUI 将生成文档，这些文档将在 web 上提供

现在，为了测试 API，创建一个以下顺序的产品 JSON，并使用以下有效负载命中一个 POST 请求:

```js
{"desc":[{"lang":"en","val":"TypescriptMicroservicesByParthGhiya."}],"name":"TypescriptMicroservices","category":"Microservices","brand":"PACKT","shipping":{"dimensions":{"height":"13.0","length":"1.8","width":"26.8"},"weight":"1.75"},"attrs":[{"name":"microservices","value":"exampleorientedbook"},{"name":"Author","value":"ParthGhiya"},{"name":"language","value":"Node.js"},{"name":"month","value":"April"}],"feedbackEmail":"ghiya.parth@gmail.com","ownerId":"parthghiya","description":"thisistestdescription"}
```

你会看到一个成功的响应与 ResponseCode: 200 和 MongoDB 的 ObjectId。 它看起来像这样:“id”:“5acac73b8bd4f146bcff9667”。

这是我们编写微服务的一般方法。 它向我们展示了更多关于控制分离的行为，如何使用 TypeScript 实现它，以及一些企业设计模式。 位于服务层的瘦控制器依赖于对业务层和数据层的引用来实现流程，通过这些流程，我们可以消除冗余代码，并在控制器之间实现服务共享。 类似地，您可以基于相同的方法编写无数的服务。 假设您想要编写一个支付微服务，您可以使用`typeorm`模块进行 SQL 操作，并且具有相同的代码结构。

# 码头工人

现在应用程序已经启动并运行，让我们对其进行容器化，以便将映像推送给任何人。 像 Docker 这样的容器可以帮助我们打包整个应用程序，包括库、依赖项、环境和应用程序运行所需要的任何其他东西。 容器很有帮助，因为它们将应用程序与基础设施隔离开来，这样我们就可以轻松地在不同的平台上运行它，而无需担心我们所运行的系统。

我们的目标是:

1.  运行`docker-compose up`运行我们的产品目录微服务的工作版本 Mongo 微服务
2.  Docker 工作流应该是我们正在使用的 Node.js 工作流，包括编译和服务`dist`文件夹
3.  使用数据容器初始化 MongoDB

让我们开始吧。 我们将创建我们的`container`文件，并通过执行以下步骤在其中编写启动脚本。 你可以在`Chapter 4/products-catalog -with-docker`文件夹中找到源代码:

1.  首先，创建`.dockerignore`文件来忽略构建容器中不想要的内容:

```js
Dockerfile
Dockerfile.dev
./node_modules
./dist
```

2.  现在，我们将写下我们的`Dockerfile`。 图像由一组我们在`Dockerfile`中定义的图层和指令组成。 我们将在这里初始化 Node.js 应用程序代码:

```js
#LATEST NODE Version -which node version u will use.
FROM node:9.2.0
# Create app directory
RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app
#install dependencies
COPY package.json /usr/src/app
RUN npm install
#bundle app src
COPY . /usr/src/app
#3000 is the port which we want to expose for outside container world.
EXPOSE 3000 
CMD [ "npm" , "start" ]
```

3.  我们完成了 Node.js 部分。 现在，我们需要配置 MongoDB。 我们将使用`docker compose`，这是一个用于运行多个容器应用程序的工具，它将使应用程序旋转并运行它。 让我们添加一个`docker-compose.yml`文件添加我们的 MongoDB:

```js
version: "2"
services:
  app:
    container_name: app
    build: ./ 
  restart: always
    ports:
      - "3000:8081"
    links:
      - mongo
  mongo:
    container_name: mongo
    image: mongo
    volumes:
      - ./data:/data/db
    ports:
      - "27017:27017"
```

Running multiple containers inside a single container is not possible as such. We would be leveraging the Docker Compose up tool ([https://docs.docker.com/compose/overview/](https://docs.docker.com/compose/overview/)) ,which can be downloaded by running `sudo curl -L https://github.com/docker/compose/releases/download/1.21.0/docker-compose-$(uname -s)-$(uname -m) -o/usr/local/bin/docker-compose`. We will look at `docker compose` in [Chapter 9](09.html), *Deployment, Logging, and Monitoring*.

对这个文件进行拆分后，我们看到以下内容:

*   我们有一个名为`app`的服务，它为产品目录服务添加一个容器。
*   我们指示 Docker 在容器失败时自动重启容器。
*   为了构建应用服务(我们的 TypeScript Node.js 应用)，我们需要告诉`Dockerfile`的位置，它可以在那里找到构建指令。 `build ./`命令告诉 Docker`Dockerfile`与`docker-compose.yml`处于同一级别。
*   我们映射主机和容器端口(这里我们保持两者相同)。
*   我们添加了另一个服务，Mongo，它从 Docker Hub 注册表中提取标准的 Mongo 图像。
*   接下来，通过挂载`/data/db`和本地数据目录`/data`定义一个数据目录。
*   这将具有类似于启动新容器的优点。 Docker compose 将使用以前容器的卷，从而确保没有数据丢失。
*   最后，我们将应用程序容器链接到 Mongo 容器。
*   端口`3000:8081`基本上告诉我们，暴露给外部容器世界的 Node.js 服务可以在端口`3000`访问，而在内部，应用程序运行在端口`8081`上。

4.  现在，在父级上打开一个终端，点击以下命令:

```js
docker-compose up
```

这将启动两个容器并聚合两个容器的日志。 现在，我们已经成功地将应用程序 Dockerized。

5\. 运行`docker-compose up`会给你一个错误，它不能连接到 MongoDB。 我们能做错什么? 我们通过`docker-compose`选项运行多个容器。 Mongo 在自己的容器中运行; 因此，它不能通过`localhost:27017`访问。 我们需要更改连接 URL，将其指向 Docker 服务，而不是本地主机。 在`default.json`中更改以下一行:

```js
"mongo":{"urlClient": "mongodb://127.0.0.1:27017/products"}, to 
"mongo":{"urlClient": "mongodb://mongo:27017/products"}
```

6\. 现在，运行`docker-compose`，您将能够成功地启动和运行服务。

通过 dockerizing 我们的微服务，我们完成了开发和构建周期。 在下一节中，在进入下一个主题*微服务最佳实践*之前，我们将快速回顾到目前为止所做的工作。

# 剧情简介

在本节中，我们将快速浏览一些我们使用过的模块，并描述它们的用途:

| `routing-controllers` | 有各种选项，基于 ES6。 它有很多的装饰，如`@GET`，`@POST`，`@PUT`，帮助我们设计无配置的服务。 |
| `config` | Config 模块，我们可以根据不同的环境编写不同的文件，从而帮助我们坚持十二个因素的应用程序。 |
| `typedi` | 用作依赖注入容器。 然后我们可以使用它将服务(`@Service`)注入到任何控制器中。 |
| `winston` | 用于日志模块。 |
| `typeORM` | 用 TypeScript 编写的用于处理关系数据库的模块。 |
| `mongoose` | 流行的 Mongoose ORM 模块处理 MongoDB。 |
| `cors` | 为我们的微服务启用 CORS 支持。 |
| `class-validator` | 用于根据我们配置的规则验证任何输入请求。 |

同样，基于这个文件夹结构和模块，我们可以创建任意数量的支持任意数据库的微服务。 现在我们已经清楚地了解了如何设计微服务，在下一节中，我们将研究一些微服务设计最佳实践。

# 微服务设计最佳实践

既然我们已经开发了一些微服务，现在是时候了解一些模式和与它们相关的一些设计决策了。 为了获得更广阔的视角，我们将着眼于微服务应该处理什么和不应该处理什么。 在设计微服务时，需要考虑许多因素，并牢记最佳实践。 微服务完全是按照单一责任的原则设计的。 我们需要界定界限，控制我们的微服务。 下面的部分涵盖了有效开发微服务需要考虑的所有因素和设计原则。

# 设置适当的微服务范围

与设计微服务相关的最重要的决策之一是微服务的大小。 规模和范围会对微服务设计产生巨大影响。 与传统方法相比，我们可以说，每个容器或执行单一职责的任何组件都应该有一个 REST 端点。 我们的微服务应该是领域驱动的，其中每个服务都绑定到该领域的特定上下文，并将处理特定的业务功能。 业务能力可以定义为为实现业务目标所做的事情或任何事情。 在我们的购物车微服务系统中，支付、添加购物车、推荐产品和发送产品是不同的业务能力。 每个不同的业务功能都应该由单独的微服务实现。 如果我们采用这种模式，我们最终将在微服务列表中包含产品目录服务、价格目录服务、发票服务、支付服务等等。 每一种技术功能，如果有的话，都应该捆绑为独立的微服务。 技术能力并不直接有助于实现业务目标，而是作为一种简化来支持其他服务。 一个示例包括集成服务。 我们应该坚持的要点可以总结为:

*   微服务应该负责单一的功能(无论是技术上的还是业务上的)
*   微服务应该是可单独部署和可扩展的
*   微服务应该易于小型团队维护，并且在任何时候都可以替换

# 自治的功能

确定微服务范围时的另一个重要因素是决定何时退出功能。 如果函数是自我维持的，也就是说，它很少依赖于外部函数，它处理给定的输出并给出一些输出。 然后，它可以作为一个微服务边界，并作为一个单独的微服务保存。 常见的例子有缓存、加密、授权、身份验证等。 我们的购物车中有许多这样的例子。 例如，它可以是一个中心日志服务，也可以是一个价格计算微服务，它接受各种输入，比如产品名称、客户折扣等等，然后在应用促销折扣(如果有的话)后计算产品的价格。

# 通晓多种语言的体系结构

产生微服务的关键需求之一是对多语言架构的支持。 不同的业务功能需要不同的处理。 “一个规则适用于任何地方”的原则已经行不通了。 需要不同的技术、体系结构和方法来处理所有的业务和技术功能。 当我们确定微服务的范围时，这是另一个需要考虑的关键因素。 例如，在我们的购物微服务系统中，产品搜索微服务不需要关系数据库，但是添加到购物车和支付服务需要符合 ACID，因为处理交易有一个非常小的需求。

# 独立可部署组件的大小

分布式微服务生态系统将充分利用目前不断增加的 CI/CD 流程实现自动化。 自动化各种步骤，例如集成、交付、部署、单元测试、扩展和代码覆盖，然后创建一个可部署单元，这会使工作变得更容易。 如果我们在单个微服务容器中包含太多内容，将会带来巨大的挑战，因为其中涉及很多过程，比如安装依赖项、自动复制文件或从 Git 下载源文件、构建、部署，然后启动。 随着微服务复杂性的增加，微服务的规模也会随之增大，这也就增加了管理微服务的难度。 设计良好的微服务可以确保部署单元仍然是可管理的。

# 在需要时分发和扩展服务

在设计微服务时，重要的是根据各种参数拆分微服务，例如深入分析哪些业务功能最受欢迎、基于所有权的服务划分、松耦合架构等等。 从长远来看，采用这种划分方式设计的微服务是有效的，因为我们可以轻松地按需扩展任何服务，并隔离故障点。 在我们的产品微服务中，大约 60%的请求是基于搜索的。 在这种情况下，我们的搜索微服务容器必须分开运行，以便在需要时可以分开伸缩。 弹性搜索或 Redis 可以引入到这个微服务之上，这将提供更好的响应时间。 这将具有各种优势，如降低成本、有效利用资源、商业效益、成本优化等。

# 在敏捷

随着需求的动态变化，敏捷开发方法已被广泛采用。 在确定微服务的范围时，一个重要的考虑因素是，以这样一种方式进行开发，即每个团队可以开发这块蛋糕的不同部分。 每个团队构建不同的微服务，然后我们构建完整的蛋糕。 例如，在我们的购物车微服务中，我们可以有一个基于用户偏好和历史的推荐服务。 这可以在保留用户的跟踪历史、浏览器历史以及其他信息的情况下进行开发，这可能会导致复杂的算法。 这就是为什么它将被开发成一个独立的微服务，可以由独立的团队来处理。

# 单一业务能力处理程序

与传统的单一责任原则有所不同，单一的微服务应该处理单一的业务能力或技术能力。 一个微服务不应该执行多个职责。 根据设计模式，可以将一个业务能力划分为多个微服务。 例如，在库存管理中的购物车微服务中，我们可以引入 CQRS 模式来实现一些质量属性，其中我们的读写将分散在不同的服务容器中。 当每个服务被映射到一个有限上下文，处理单个业务功能时，管理它们就容易得多。 每个服务都可能作为单独的产品存在，针对特定的社区。 它们应该是可重用的，容易部署的，等等。

# 适应不断变化的需求

微服务的设计应该使它们能够轻松地与系统分离，而重写的次数最少。 这使我们能够轻松地添加实验特性。 例如，在我们的购物车微服务中，我们可以根据收到的反馈添加产品排名服务。 如果服务不能工作，或者业务功能不能实现，则可以丢弃该服务，或者很容易用另一个服务替换该服务。 在这里，确定微服务的范围扮演着重要的角色，因为可以制作出最小的可行产品，然后在此基础上，可以根据需求添加或删除功能。

# 处理依赖关系和耦合

确定服务范围的另一个重要因素是服务引入的依赖关系和耦合。 必须评估微服务中的依赖关系，以确保系统中没有引入紧耦合。 为了避免高耦合系统，将系统分解为业务/技术/功能功能，并创建一个功能依赖树。 太多的请求-响应调用、周期性依赖关系等等都是可能破坏微服务的一些因素。 设计健壮的微服务的另一个重要方面是拥有事件驱动的架构; 也就是说，微服务不是等待响应，而是在接收事件时作出反应。

# 决定微服务中的端点数量

虽然这似乎是设计微服务的一个重要考虑点，但它根本不是一个设计考虑。 一个微服务容器可以承载一个或多个端点。 一个更重要的考虑是限制微服务。 基于业务或技术功能，可能只有一个端点，而在许多情况下，微服务中可能有多个端点。 例如，回到购物车服务，在我们的库存管理中引入了 CQRS 模式，我们有单独的读和写服务，每个服务包含单个端点。 另一个例子是多语言架构，其中我们有多个端点，以便在各种微服务之间进行通信。 我们通常根据部署和扩展需求将服务分解为多个容器。 对于我们的结帐服务，所有服务都连接起来并使用相同的关系数据库。 在这种情况下，没有必要将这些分离到不同的微服务中。

# 微服务之间的通信风格

设计微服务时要考虑的另一个重要因素是微服务之间的通信风格。 可以是同步模式(发送请求、接收响应)，也可以是异步模式(发送并忘记)。 这两种模式都有各自的优缺点，并且都有各自的特定用例。 为了拥有可扩展的微服务，需要结合这两种方法。 除此之外，现在“实时”是新的趋势。 基于套接字的通信促进了实时通信。 另一种划分通信方式的方法是根据接收人的数量。 对于单个接收器，我们有一个基于命令的模式(CQRS，如前面章节所见)。 对于多个接收器，我们有一个基于发布和订阅模式原则的事件驱动架构，其中使用了服务内总线。

# 指定和测试微服务契约

契约可以定义为使用者和提供者之间的一组协议(协议、请求正文、地址等等)，这有助于平滑它们之间发生的交互。 微服务的设计应该使它们能够独立部署，彼此之间没有任何依赖关系。 为了实现这种完全的独立性，每个微服务都应该有良好的编写、版本控制和定义的契约，它的所有客户端(其他微服务)都必须遵守这些契约。 在任何时候引入破坏的更改可能是一个问题，因为客户可能需要以前的合同版本。 只有在适当的沟通之后，合同才会被终止或拒绝。 一些最佳实践包括并排部署新版本，并在 API 中包含版本控制。 例如，`/product-service/v1`，然后`/product-service/v2`。 与集成测试相比，使用**消费者驱动的契约**(**cdc**)是测试微服务的一种现代方法。 此外，在本书中，我们将使用 Pact JS 来测试我们的合同([第 8 章](08.html)、*测试、调试和文档*)。

# 容器中的微服务数量

容器化微服务是部署微服务最推荐的方法之一。 容器在您的系统中提供了敏捷性，它简化了开发和测试经验。 容器可以跨任何基础设施移植，也可以轻松地部署在 AWS 上。 决定容器可以包含的微服务数量非常重要，这取决于各种因素，如容器容量、内存、选择性伸缩、资源需求、每个服务的通信量等等。 根据这些事实，我们可以决定部署是否可以合并在一起。 即使将服务放在一起，也必须确保这些服务是独立运行的，并且它们没有共享任何东西。 选择伸缩也是决定容器中微服务数量的关键因素之一。 部署应该是自我管理的，AWS Lambda 就是一个例子。 以下是可用的模式和每种模式的限制:

*   **每个虚拟机的服务实例**:**这里，您将每个服务打包为一个虚拟机映像(传统方法)，例如 Amazon EC2 EMI。 这里，每个服务都是一个独立的虚拟机，在一个独立的虚拟机映像中启动:

    *   **限制**:
    *   资源利用效率较低
    *   你支付整个 VM; 因此，如果您没有使用整个 VM，那么您是在免费支付费用
    *   在服务上部署新版本非常缓慢
    *   管理多个 vm 很快就会成为一项巨大的痛苦和耗时的活动** 
***   **每个容器的服务实例**:**这里，每个服务运行在自己的容器上。 容器是一种可移植的虚拟化技术。 它们有自己的根文件系统和可移植的名称空间。 您可以限制它们的 CPU 资源和内存。

    *   **限制**:
    *   容器不如 VM 成熟
    *   处理负载中的峰值是一项额外的任务
    *   监控虚拟机基础设施和容器基础设施再次增加了一个任务** ***   **无服务器**:**最新的“无服务器趋势”之一是无服务器架构，你打包一个微服务，打包成 ZIP，然后部署到无服务器平台上，比如 AWS Lambda。 您只是根据所花费的时间和内存消耗来为每个请求计费。 例如，Lambda 函数是无状态的。

    *   **限制:**
    *   这种方法对于长期运行的服务是不可行的。 例如，一个服务依赖于另一个服务或第三方代理。
    *   请求必须在少于 300 秒内完成。
    *   服务必须是无状态的，因为每个请求都要运行每个单独的实例。
    *   服务必须快速启动，否则将超时。
    *   服务必须以一种受支持的语言运行。 例如，AWS Lambda 支持 Java、Node.js 和 Python。****** 

 ****# 微服务中的数据源和规则引擎

另一个重要因素是在我们的分布式系统中应用规则引擎并决定数据源。 规则是任何系统的重要组成部分，因为它帮助我们管理整个系统。 许多组织使用集中式规则引擎或工作流流程，遵循 BPMN 标准示例 Drools。 根据使用情况，嵌入式规则引擎可以放在服务内部，也可以放在服务外部。 如果有复杂的规则，带嵌入式引擎的中央创作存储库将是最佳选择。 由于它是集中分布的，因此可能存在技术依赖关系、在某些应用服务器边界内运行规则的规则，等等。

**Business Process Modeling Notations** (**BPMN**) are standardized notations with the objective of creating visual models of any business or any organizational process. Often in business capabilities, we need a definitive workflow that may change as per requirements. We never hardcode any processes or write our own engine and leverage BPMN tools for it.

就像规则引擎一样，决定微服务之间的数据存储也是至关重要的。 应该在我们定义的业务能力中设置事务边界。 例如，在我们的购物车微服务中，在结帐时我们需要维护事务，我们可以使用 RDBMS 作为数据源来确保完整性并遵循 ACID 原则。 然而，产品目录数据库没有任何事务，我们可以为它们使用 NoSQL 数据库。

# 总结

在本章中，我们开始为购物车服务设计我们的微服务。 我们基于技术、功能和业务能力分析了我们的需求，这些是确定微服务范围的主要驱动因素。 我们设计了我们的模式，分析了我们的微服务结构，并在 Docker 上运行。 最后，我们研究了微服务设计的一些最佳实践，并了解了如何根据我们的业务功能确定微服务的范围。

在下一章中，我们将学习如何将网关引入我们的微服务，并了解网关解决的问题。 我们将看到 API Gateway 如何解决分布式系统中的集中问题。 我们将了解一些 API 网关设计模式，并为购物车微服务设计我们的网关。**********