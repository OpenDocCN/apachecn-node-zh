Chapter 8. Deploying Microservices <link rel="stylesheet" href="epub.css" type="text/css"> 

# 第八章。 部署 Microservices

在本章中，我们将部署微服务。 我们将使用不同的技术，以便为读者提供为每个工作选择合适工具所需的知识。 首先，我们将使用 PM2 及其部署功能在远程服务器中运行应用程序。 然后，我们将使用 Docker，这是最先进的部署平台之一，以及围绕容器的整个生态系统。 在本章中，我们将展示如何尽可能高地自动化所有部署。

# 软件部署的概念

部署通常是**软件开发生命周期**(**SDLC**)聚会的丑朋友。 在开发和系统管理之间有一个缺失的接触点，DevOps 将在未来几年内解决这个问题。 SDLC 不同阶段修复 bug 的成本如下图所示:

![Concepts in software deployment](graphics/B04889_08_01.jpg)

这张图显示了修复漏洞的成本，取决于 SDLC 的阶段

早期失败是精益方法论中我最喜欢的概念之一。 在变更管理领域中，在软件生命周期的不同阶段中修复 bug 的成本被称为**变更成本曲线**。

粗略估计，在生产环境中修复一个 bug 的成本是在满足需求时修复它的成本的 150 倍。

不管这个数字是多少，它在很大程度上取决于我们使用的方法和技术，从中得到的教训是，通过尽早捕获 bug，我们可以节省大量时间。

从持续集成到持续交付，流程应该尽可能地自动化，其中*尽可能多*意味着 100%自动化。 记住，人类是不完美的，在执行手工重复性任务时更容易出错。

## 持续集成

**持续集成**(**CI)的集成来自不同部门的工作实践每天(或超过一天一次)和验证更改不失现有功能集成和运行单元测试。**

CI 应该使用与我们稍后在预生产和生产中使用的相同的基础结构配置来自动化，所以如果有任何缺陷，可以及早发现。

## 连续交付

**连续交付**(**CD**)是一种软件工程方法，旨在构建小型的、可测试的、且易于部署的功能块，可以在任何时候无缝交付。

这就是我们在微服务方面的目标。 同样，我们应该推动交付过程的自动化，因为如果我们手动做，我们只是在寻找问题。

从微服务的角度来看，部署的自动化是关键。 我们需要解决拥有几十个服务而不是几台机器的开销，或者我们可以发现自己在维护一个云服务，而不是为我们的公司增加价值。

**Docker**是我们在这里最好的盟友。 有了 Docker，我们可以减少部署一个新软件在不同环境中移动文件(容器)的麻烦，我们将在本章的后面看到这一点。

# PM2 部署

PM2 是一个非常强大的工具。 无论我们处于的哪个发展阶段，PM2 总能提供一些东西。

在软件开发的这个阶段，部署是 PM2 真正发挥作用的地方。 通过 JSON 配置文件，PM2 将管理应用程序集群，以便我们可以轻松地在远程服务器上部署、重新部署和管理应用程序。

## PM2 -生态系统

PM2 称一组应用生态系统。 每个生态系统都由一个 JSON 文件描述，生成它的最简单方法是执行以下命令:

```js
pm2 ecosystem

```

这应该输出类似以下代码的内容:

```js
[PM2] Spawning PM2 daemon
[PM2] PM2 Successfully daemonized
File /path/to/your/app/ecosystem.json generated

```

根据 PM2 版本的不同，`ecosystem.json`文件的内容会有所不同，但是这个文件包含的是 PM2 集群的框架:

```js
{
  apps : [

    {
      name      : "My Application",
      script    : "app.js"
    },

    {
      name      : "Test Web Server",
      script    : "proxy-server.js"
    }
  ],

*/
  deploy : {
    production : {
      user : "admin",
      host : "10.0.0.1",
      ref  : "remotes/origin/master",
      repo : "git@github.com:the-repository.git",
      path : "/apps/repository",
      "post-deploy" : "pm2 startOrRestart ecosystem.json --env production"
    },
    dev : {
      user : "devadmin",
      host : "10.0.0.1",
      ref  : "remotes/origin/master",
      repo : "git@github.com:the-repository.git",
      path : "/home/david/development/test-app/",
      "post-deploy" : "pm2 startOrRestart ecosystem.json --env dev",
    }
  }
}
```

该文件包含为两个环境配置的两个应用程序。 我们将修改这个框架，使其适应我们的需要，建模我们的整个生态系统写在[第 4 章](4.html "Chapter 4. Writing Your First Microservice in Node.js")，*写你的第一个微服务在 Node.js*。

然而，现在，让我们解释一下配置:

*   我们有一个应用程序数组(`apps`)，它定义了两个应用程序:API 和 WEB
*   如你所见，我们为每个应用程序提供了一些配置参数:
    *   `name`:这是应用的名称
    *   `script`:这是应用的启动脚本
    *   `env`:这些是 PM2 要注入到系统中的环境变量
    *   `env_<environment>`:这与`env`相同，但它是针对每个环境量身定制的
*   `deploy`键下的默认生态系统中定义了两个环境，如下:
    *   `production`
    *   `dev`

如您所见，在这两个环境之间，除了我们在开发中配置一个环境变量和部署应用程序的文件夹之外，没有什么重大变化。

## 使用 PM2 部署微服务

在[第四章](4.html "Chapter 4. Writing Your First Microservice in Node.js")、*用 Node.js*编写你的第一个微服务，为了展示微服务中不同的概念和常见的捕获，我们编写了一个简单的电子商务。

现在，我们将学习如何使用 PM2 来部署它们。

### 配置服务器

为了使用 PM2 部署软件，我们需要做的第一件事是配置远程机器和本地机器，使其能够使用 SSH 和一个公钥/私钥模式进行通信。

做这件事的方法很简单，如下:

*   生成一个 RSA 密钥
*   将其安装到远程服务器中

让我们做到:

```js
ssh-keygen -t rsa

```

这应该产生类似以下输出:

```js
Generating public/private rsa key pair.
Enter file in which to save the key (/Users/youruser/.ssh/id_rsa): /Users/youruser/.ssh/pm2_rsa
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in pm2_rsa.
Your public key has been saved in pm2_rsa.pub.
The key fingerprint is:
eb:bc:24:fe:23:b2:6e:2d:58:e4:5f:ab:7b:b7:ee:38 dgonzalez@yourmachine.local
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|                 |
|                 |
|    .            |
|   o    S        |
|    o   ..       |
|   o o..o.       |
|  . +.+=E..      |
|   oo++**B+.     |
+-----------------+

```

现在，如果我们进入前面输出中显示的文件夹，我们可以找到以下两个文件:

*   `pm2_rsa`:第一个`pm2_rsa`是您的私钥。 从名称中可以看出，没有人可以访问此密钥，因为他们可能会在信任此密钥的服务器中窃取您的身份。
*   `pm2_rsa.pub`:`pm2_rsa.pub`是您的公钥。 这个密钥可以交给任何人，因此使用非对称加密技术，他们可以验证你的身份(或你说你是谁)。

我们现在要做的是将公钥复制到远程服务器，这样当我们的本地机器 PM2 试图与服务器对话时，它就知道我们是谁，让我们不需要密码就进入 shell:

```js
cat pm2_rsa.pub | ssh youruser@yourremoteserver 'cat >> .ssh/authorized_keys'

```

最后一步是在你的本地机器上注册你的私钥作为一个已知的身份:

```js
ssh-add pm2_rsa

```

仅此而已。

从现在开始，当您以用户`youruser`的身份 SSH 进入远程服务器时，您将不需要输入密码就可以进入 shell。

配置完成后，要将任何应用程序部署到该服务器上，需要做的事情就很少了:

```js
pm2 deploy ecosystem.json production setup
pm2 deploy ecosystem.json production

```

第一个命令将配置应用程序所需的所有内容。第二个命令将实际部署应用程序本身，就像我们之前配置的那样。

# Docker -一个软件交付的容器

虚拟化是过去几年来最大的趋势之一。 虚拟化使工程师能够在不同的软件实例之间共享硬件。 Docker 并不是一个真正的虚拟化软件，但在概念上是相同的。

通过纯虚拟化解决方案，新的操作系统运行在现有操作系统(主机操作系统)之上的管理程序之上。 运行完整的操作系统意味着，为了将整个堆栈从内核复制到文件系统，我们可能会消耗几 gb 的硬盘驱动器，这通常会消耗大量的资源。 虚拟化解决方案的结构如下图所示:

![Docker – a container for software delivery](graphics/B04889_08_02.jpg)

虚拟机环境的层图

使用 Docker，我们只需要复制文件系统和二进制文件，这样就不需要在不需要的地方运行整个操作系统堆栈。 Docker 映像通常是几百兆字节，而不是千兆字节，而且它们是相当轻量的，因此，我们可以在同一台机器上运行一些容器。 前面使用 Docker 的结构如下所示:

![Docker – a container for software delivery](graphics/B04889_08_03.jpg)

Docker 的层图

有了 Docker，我们还消除了软件部署的一个最大问题，即****配置管理**。**

 **我们正在切换一个复杂的每个环境的配置管理，我们需要担心如何将应用部署/配置到一个容器中，这个容器基本上就像一个软件包，可以安装在任何 docker 就绪的机器上。

目前唯一可以使用 Docker 的操作系统是 Linux，因为 Docker 需要利用高级内核特性，迫使 Windows 和 Mac 用户在 Linux 上运行虚拟机，以提供运行 Docker 容器的支持。

## 设置容器

Docker提供了一种非常强大和熟悉的(对于开发人员)配置容器的方式。

您可以基于现有映像(Internet 上有数千个映像)创建容器，然后通过添加新的软件包或更改文件系统来修改映像以满足您的需要。

一旦我们对它感到满意，我们就可以使用新版本的映像来创建容器，使用类似于**Git**的版本控制系统。

然而，我们首先需要了解 Docker 是如何工作的。

### 安装 Docker

正如前面提到的，Docker 需要一个虚拟机在 Mac 和 Windows 上提供支持，所以在这些系统上的安装可能会有所不同。 在你的系统上安装 Docker 的最好方法是进入官方网站，按照的步骤:

[https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)

目前，这是一个非常活跃的项目，所以您可以期待每隔几周就会发生变化。

### 选择图像

默认情况下，Docker没有图像。 我们可以通过在终端上运行`docker images`来验证这一点，这将产生一个非常类似以下截图的输出:

![Choosing the image](graphics/B04889_08_04.jpg)

它是一个空列表。 本地机器中没有存储映像。 我们需要做的第一件事是搜索一张图片。 在本例中，我们将使用**CentOS**作为创建映像的基础。 CentOS 非常接近 Red Hat Enterprise Linux，后者似乎是行业中扩展最广的 Linux 发行版之一。 它们提供了大量的支持，并且在 Internet 上有大量的信息可以用来排除问题。

让我们搜索一个 CentOS 映像如下:

![Choosing the image](graphics/B04889_08_05.jpg)

正如您可以看到的，有一长串基于 CentOS 的映像，但只有第一个是官方的。

这个图片列表来自于 Docker 世界中称为**注册**的东西。 Docker Registry 是一个简单的映像存储库，供普通大众使用。 您也可以运行您自己的注册表，以防止您的图像转到一般的一个。

### 注意事项

更多信息可在以下链接找到:

[https://docs.docker.com/registry/](https://docs.docker.com/registry/)

在前面的截图中，表格中有一个列应该会立刻引起你的注意，这个列叫做**STARS**。 这一列表示用户对给定图像的评价。 通过使用`-s`标志，我们可以根据用户提供给图像的星星数量缩小搜索范围。

如果你运行下面的命令，你会看到一个 1000 或更多星级的图片列表:

```js
docker search -s 1000 centos

```

### 提示

小心您选择的映像，没有什么可以阻止用户使用恶意软件创建映像。

为了获取 CentOS 映像到本地机器，我们需要运行以下命令:

```js
docker pull centos

```

产生的输出将非常类似于以下图像:

![Choosing the image](graphics/B04889_08_06.jpg)

一旦命令完成，如果我们再次运行 Docker 映像，我们可以看到**centos**现在出现在以下列表中:

![Choosing the image](graphics/B04889_08_07.jpg)

正如我们前面提到的，Docker 并不使用完整的镜像，但它使用了镜像的简化版本，只虚拟化了操作系统的最后几层。 您可以清楚地看到，映像的大小甚至不是 200 MB，而对于一个完整版的 CentOS 来说，可以达到几个 GB。

### 运行容器

现在在我们的本地机器中有一个映像的副本，是时候运行它了:

```js
docker run -i -t centos /bin/bash

```

这将产生以下输出:

![Running the container](graphics/B04889_08_08.jpg)

如您所见，终端的提示符已更改为类似`root@debd09c7aa3b`的内容，这意味着我们在集装箱内。

从现在开始，我们运行的每一个命令都将在一个包含的 CentOS Linux 版本中执行。

在 Docker 中还有另一个有趣的命令:

```js
docker ps

```

如果我们在一个新的终端中运行这个命令(不从运行的容器中退出)，我们将得到以下输出:

![Running the container](graphics/B04889_08_09.jpg)

这个输出是不言自明的; 这是查看 Docker 容器中正在发生什么事情的一种简单方法。

### 安装所需软件

让我们在容器中安装Node.js:

```js
curl --silent --location https://rpm.nodesource.com/setup_4.x | bash -

```

这个命令将拉动并执行 Node.js 的设置脚本。

这将产生一个非常类似下图的输出:

![Installing the required software](graphics/B04889_08_29.jpg)

按照的说明，这将安装节点:

```js
yum install -y nodejs

```

强烈建议安装开发工具，因为几个模块的安装过程需要一个编译步骤。 让我们做到:

```js
yum install -y gcc-c++ make

```

一旦命令完成，我们就可以在容器中运行节点应用程序了。

### 保存更改

在 Docker 世界中，映像是给定容器的配置。 我们可以使用映像作为模板来运行任意数量的容器，但是首先，我们需要保存上一节中所做的更改。

如果您是一名软件开发人员，您可能熟悉控制版本系统，如 CVS、Subversion 或 Git。 Docker 是基于他们的理念构建的——容器可以被视为一个可版本化的软件组件，然后可以提交更改。

为了做到这一点，运行以下命令:

```js
docker ps -a

```

这个命令将显示过去运行过的容器列表，如下图所示:

![Saving the changes](graphics/B04889_08_10.jpg)

在我的例子中，容器很少，但在这个例子中有趣的是第二个; 这是安装Node.js 的地方。

现在，我们需要提交容器的状态，以便使用更改创建一个新映像。 我们通过运行以下命令来实现:

```js
docker commit -a dgonzalez 62e7336a4627 centos-microservices:1.0

```

让我们来解释一下这个命令:

*   `-a`标志表示作者。 在这种情况下，`dgonzalez`。
*   以下参数为`container id`。 如前所述，第二个集装箱对应的 ID 为`62e7336a4627`。
*   第三个参数是新图像的名称和图像的标记的组合。 当我们处理相当多的图像时，标签系统可以非常强大，因为识别它们之间的小差异会变得非常复杂。

这可能需要几秒钟，但在完成之后，命令的输出必须非常类似于下面的图像:

![Saving the changes](graphics/B04889_08_11.jpg)

这表明我们在安装了软件的列表中有了一个新映像。 再次运行`docker images`，输出将确认它，如下图所示:

![Saving the changes](graphics/B04889_08_12.jpg)

为了运行基于新映像的容器，我们可以运行以下命令:

```js
docker run -i -t centos-microservices:1.0 /bin/bash

```

这将使使我们能够访问容器中的 shell，并且我们可以通过运行`node -v`来确认 Node.js 已经安装，它应该输出 Node 的版本，在本例中是 4.2.4。

## 部署 Node.js 应用

现在，是时候在容器中部署 Node.js 应用程序了。 为了做到这一点，我们需要将本地机器的代码暴露给 Docker 容器。

正确的方法是在 Docker 机器上挂载一个本地文件夹，但首先，我们需要创建一个小应用程序在容器中运行，如下所示:

```js
var express = require('express');
var myApplication = express();

app.get('/hello', function (req, res) {
  res.send('Hello Earth!');
});

var port = 80;

app.listen(port, function () {
  console.log('Listeningistening on port '+ port);
});
```

它是一个简单的应用程序，使用 Express 将`Hello Earth!`基本呈现到浏览器中。 如果我们从终端运行它并访问`http://localhost:80/hello`，我们可以看到结果。

现在，我们要在容器中运行它。 为了做到这一点，我们将在 Docker 容器中挂载一个本地文件夹作为卷并运行它。

Docker 来自于系统管理员和开发人员的经验，他们最近融入了一个叫做 DevOps 的角色，这个角色介于两者之间。 在 Docker 之前，每个公司都有自己的部署应用和管理配置的方式，所以在如何正确地做事情上没有共识。

现在有了 Docker，两家公司就有办法提供统一部署。 无论您的业务是什么，都可以简化为构建容器、部署应用程序和在适当的机器上运行容器。

我们假设应用程序位于`/apps/test/`文件夹中。 现在，为了将其暴露给容器，我们运行以下命令:

```js
docker run -i -t -v /app/test:/test_app -p 8000:3000 centos-microservices:1.0 /bin/bash

```

正如你可以看到的，Docker 使用参数会变得非常冗长，但让我们解释一下，如下:

*   `-i`和`-t`标志为我们所熟悉。 它们捕获输入并将输出发送到终端。
*   `-v`标志是新的。 它指定来自本地机器的卷以及将它挂载到容器中的何处。 在这种情况下，我们将从本地机器安装`/apps/test`到`/test_app`。 请注意用冒号分隔本地路径和远程路径。
*   `-p`标志指定将在容器中公开远程端口的本地机器上的端口。 在本例中，我们通过 Docker 机器的`8000`端口公开容器中的`3000`端口，因此从主机访问`docker-machine:8000`将最终访问容器中的`3000`端口。
*   `centos-microservices:1.0`是我们在上一节中创建的图像的名称和标记。
*   `/bin/bash`是我们想要在容器中执行的命令。 `/bin/bash`将给我们进入系统提示符的权限。

如果一切正常，我们应该可以访问容器内部的系统提示符，如下图所示:

![Deploying Node.js applications](graphics/B04889_08_13.jpg)

正如你在图片中看到的，有一个名为`/test_app`的文件夹，其中包含我们之前的应用程序`small-script.js`。

现在，是时候访问该应用程序了，但首先，让我们解释一下 Docker 是如何工作的。

Docker 是用**Go**编写的，它是由谷歌创建的一种现代语言，将编译语言(如 c++)的所有优点与现代语言(如 Java)的所有高级特性结合在一起。

学起来相当容易，掌握起来也不难。 Go 的理念是将解释语言的所有优点，比如减少编译时间(一分钟内就可以编译完整的语言)，转化为编译语言。

Docker 使用了来自 Linux 内核的非常特殊的特性，迫使 Windows 和 Mac 用户使用虚拟机来运行 Docker 容器。 该机器过去被称为**boot2docker**，但新版本被称为**Docker machine**，它包含更多高级特性，如在远程虚拟机中部署容器。 对于本例，我们将只使用本地功能。

如果在`/test_app/`文件夹中的容器中运行应用程序，并且在 Linux 中访问`http://localhost:8000/`，那么就足够进入应用程序了。

当你使用 Mac 或 Windows 时，Docker 运行在 Docker Machine 或 boot2docker 中，因此 IP 由这个虚拟机给出，这在 Docker 终端启动时显示，如下图所示:

![Deploying Node.js applications](graphics/B04889_08_14.jpg)

如您所见，IP 是`192.168.99.100`，因此为了访问我们的应用程序，我们需要访问`http://192.168.99.100:9000/`URL。

## 自动创建 Docker 容器

如果你还记得，在前面的章节中，一个最重要的概念是自动化。 在使用微服务时，自动化是关键。 您可能需要操作几十台服务器，而不是操作一台服务器，从而达到几乎每天都要完成任务的程度。

Docker 的设计者在允许用户从一个名为`Dockerfile`的文件中写入的脚本创建容器时就考虑到了这一点。

如果您曾经编写过 C 或 c++，甚至在大学时，您可能对`Makefiles`很熟悉。 `Makefile`文件是一个脚本，开发人员在其中指定自动构建软件组件的步骤。 下面是的一个例子:

```js
all: my-awesome-app

my-awesome-app: main.o external-module.o app-core.o
  g++ main.o external-module.o app-core.o -o my-awesome-app

main.o: main.cc
  g++ -c main.cc

external-module.o: external-module.cc
  g++ -c external-module.cc

app-core.o: app-core.cc
  g++ -c hello.cc

clean:
  rm *.o my-awesome-app
```

前面的`Makefile`包含要执行的任务和依赖项的列表。 例如，如果我们在包含`Makefile`文件的同一文件夹上执行`make clean`，它将删除可执行文件和以`o`结尾的所有文件。

与`Makefile`不同，`Dockerfile`不是任务和依赖项的列表(尽管概念是相同的)，它是一个从从头构建容器到就绪状态的指令列表。

让我们看一个例子:

```js
FROM centos
MAINTAINER David Gonzalez
RUN curl --silent --location https://rpm.nodesource.com/setup_4.x | bash -
RUN yum -y install nodejs

```

前面的这几行代码足以构建一个安装了 Node.js 的容器。

让我们在下面解释一下:

*   首先，我们选择基本图像。 在本例中，它是我们之前使用的`centos`。 为此，我们使用`FROM`命令，然后使用图像的名称。
*   指定创建容器的人员的名称。 在本例中，它是`David Gonzalez`。
*   `RUN`，顾名思义，是运行命令。 在本例中，我们使用了两次:一次是将存储库添加到`yum`，然后是安装 Node.js。

Dockerfiles 可以包含许多不同的命令。 它们的文档非常清楚，但让我们看看最常见的(除了之前看到的):

*   `CMD`:与 run 相似的是，但它实际上是在构建命令之后执行的。 `CMD`是在容器实例化后用来启动应用程序的命令。
*   `WORKDIR`:这个将与`CMD`一起使用，它是用来为下一个`CMD`命令指定工作目录的命令。
*   `ADD`:命令用于将文件从本地文件系统复制到容器实例文件系统。 在前面的示例中，我们可以使用`ADD`将应用程序从主机复制到容器中，使用`CMD`命令运行`npm install`，然后使用`CMD`命令再次运行该应用程序。 它还可以用于将内容从 URL 复制到容器内的目标文件夹。
*   `ENV`:用于设置环境变量。 例如，您可以通过向容器传递一个环境变量来指定一个文件夹来存储上传的文件，如下所示:
*   `EXPOSE`:这是，用于将端口暴露给集群中的其他容器。

如您所见，`Dockerfiles`的**域特定语言**(**DSL**)非常丰富，您几乎可以构建所需的每个系统。 互联网上有成百上千的例子可以构建几乎所有的东西:MySQL、MongoDB、Apache 服务器等等。

强烈建议通过`Dockerfiles`创建容器，因为它可以作为脚本在未来复制和更改容器，以及能够自动部署我们的软件，而不需要太多的手动干预。

# Node.js 事件循环——容易学习，难掌握

我们都知道 Node.js 是一个以单线程方式运行应用的平台; 那么，为什么我们不使用多线程来运行应用程序呢?这样我们就可以享受多核处理器的好处了。

Node.js 是在一个名为**libuv**的库上构建。 这个库抽象系统调用，为使用它的程序提供异步接口。

我来自一个 Java 背景很重,在那里,一切都是同步(除非你是编码与某种非阻塞库),如果你发出请求到数据库,数据库线程被阻塞,恢复一次回复的数据。

这通常工作得很好，但它带来了一个有趣的问题:一个被阻塞的线程正在消耗可以用于服务其他请求的资源。 Node.js 的事件循环如下图所示:

![Node.js event loop – easy to learn and hard to master](graphics/B04889_08_15.jpg)

这是 Node.js 事件循环图

默认情况下，JavaScript 是一种事件驱动语言。 它执行一个程序，该程序配置一个事件处理程序列表，这些事件处理程序将对给定的事件作出反应，然后，它只等待动作发生。

让我们来看一个非常熟悉的例子:

```js
<div id="target">
  Click here
</div>
<div id="other">
  Trigger the handler
</div>
```

JavaScript 代码如下:

```js
$( "#target" ).click(function() {
  alert( "Handler for .click() called." );
});
```

如您所见，这是一个非常简单的示例。 HTML，显示按钮和 JavaScript 代码片段，使用 JQuery，在按钮被单击时显示一个警告框。

这是点击按钮时的键:*。*

单击按钮就是一个事件，并且使用 JavaScript 中指定的处理程序通过 JavaScript 的事件循环来处理该事件。

在一天结束的时候，我们只有一个线程在执行事件，而且我们在 JavaScript 中从不讨论并行性，正确的词是并发性。 因此，更简洁地说，我们可以说 Node.js 程序是高度并发的。

您的应用程序将始终只在一个线程中执行，我们在编码时需要记住这一点。

如果您使用过 Java 或. net 或任何其他使用线程阻塞技术设计和实现的语言/框架，那么您可能已经注意到，在运行应用程序时，Tomcat 会生成许多线程来侦听请求。

在 Java 世界中，每个线程都是，称为**worker**，它们负责从始至终处理来自给定用户的请求。 Java 中有一种数据结构可以从中受益。 它被称为**ThreadLocal**，它将数据存储在本地线程中，以便稍后可以恢复。 这种类型的存储是可能的，因为启动请求的线程也负责完成它，如果线程正在执行任何阻塞操作(例如读取文件或访问数据库)，它将等待直到它完成。

这通常不是什么大问题，但是当您的软件严重依赖于 I/O 时，问题就会变得很严重。

另一个支持 Node.js 的非阻塞模型的要点是缺少上下文切换。

CPU 开关与另一个线程时,发生了什么是,所有寄存器中的数据,以及其他地区的记忆,是堆叠,并允许 CPU 上下文切换的新工艺有其自己的数据被放置在那里,如下图所示:

![Node.js event loop – easy to learn and hard to master](graphics/B04889_08_16.jpg)

这张图从理论的角度显示了线程中的上下文切换。

此操作需要时间，并且此时间不会被应用程序使用。 它只是迷失了。 在 Node.js 中，你的应用程序只在一个线程中运行，所以在运行时没有这样的上下文切换(它仍然存在于后台，但对你的程序隐藏)。 在下图中，我们可以看到当 CPU 切换一个线程时，在现实世界中会发生什么:

![Node.js event loop – easy to learn and hard to master](graphics/B04889_08_17.jpg)

这张图从实际(显示了死时间)的角度显示了线程中的上下文切换。

# 集群 Node.js 应用

到目前为止，您已经知道了 Node.js 应用程序是如何工作的，当然，一些读者可能会有一个问题，如果应用程序运行在单个线程上，那么现代的多核处理器会发生什么?

在回答这个问题之前，让我们看一下下面的场景。

当我还在上高中的时候，cpu 有一个巨大的技术飞跃:细分。

这是第一次尝试在指令级引入并行性。 正如你可能知道的，CPU 解释汇编指令，每一个这些指令都由许多阶段组成，如下图所示:

![Clustering Node.js applications](graphics/B04889_08_18.jpg)

在Intel 4x86 之前，CPU 一次只执行一条指令，所以从上图的指令模型来看，任何 CPU 每 6 个 CPU 周期只能执行一条指令。

然后，分割开始发挥作用。 通过一组中间寄存器，CPU 工程师设法并行化指令的各个阶段，以便在最好的情况下，CPU 能够每个周期(或几乎)执行一条指令，如下图所示:

![Clustering Node.js applications](graphics/B04889_08_19.jpg)

图像描述了在 CPU 中使用分段管道执行指令的过程

这种技术改进使 cpu 速度更快，并为本地硬件多线程打开了大门，这导致了现代 n 核处理器可以执行大量的并行任务，但当我们运行 Node.js 应用程序时，我们只使用一个核。

如果我们不集群我们的应用，我们将会有一个严重的性能下降与其他平台相比，利用多核 CPU 的优势。

然而，这一次我们很幸运，PM2 已经允许你对 Node.js 应用进行集群，从而最大限度地利用你的 cpu。

另外，PM2 的一个重要方面是，它允许您在不停机的情况下扩展应用程序。

让我们在集群模式下运行一个简单的应用程序:

```js
var http = require("http");
http.createServer(function (request, response) {
  response.writeHead(200, {
    'Content-Type': 'text/plain'
  });
  response.write('Here we are!')
  response.end();
}).listen(3000);
```

这一次，我们使用了 Node.js 的原生 HTTP 库来处理传入的 HTTP 请求。

现在我们可以从终端运行应用程序，看看它是如何工作的:

```js
node app.js

```

虽然它不输出任何内容，但我们可以 curl 到`http://localhost:3000/`URL，以便查看服务器如何响应，如下面的截图所示:

![Clustering Node.js applications](graphics/B04889_08_20.jpg)

正如你可以看到的，Node.js 已经管理了所有的 HTTP 协商，并且它也已经管理了用`Here we are!`短语作为响应，因为它是在代码中指定的。

这个服务非常简单，但它是更复杂的 web 服务工作的原则，因此我们需要将 web 服务集群起来以避免瓶颈。

Node.js 有一个名为`cluster`的库，它允许我们以编程方式对应用进行集群，如下所示:

```js
var cluster = require('cluster');
var http = require('http');
var cpus = require('os').cpus().length;

// Here we verify if the we are the master of the cluster: This is the root process
// and needs to fork al the childs that will be executing the web server.
if (cluster.isMaster) {
  for (var i = 0; i < cpus; i++) {
    cluster.fork();
  }

  cluster.on('exit', function (worker, code, signal) {
    console.log("Worker " + worker.proces.pid + " has finished.");
  });
} else {
  // Here we are on the child process. They will be executing the web server.
  http.createServer(function (request, response) {
    response.writeHead(200);
    response.end('Here we are!d\n');
  }).listen(80);
}
```

就我个人而言，我发现使用特定的软件(如 PM2)来实现有效的集群要容易得多，因为在处理应用程序的集群实例时，代码会变得非常复杂。

因此，我们可以通过 PM2 运行应用程序，如下所示:

```js
pm2 start app.js -i 1

```

![Clustering Node.js applications](graphics/B04889_08_21.jpg)

正如您可以在命令的输出中看到的，PM2 中的`-i`标志用于指定我们希望用于应用程序的内核数量。

如果我们运行`pstree`，我们可以在我们的系统中看到进程树，并检查 PM2 是否只运行我们的应用程序的一个进程，如下图所示:

![Clustering Node.js applications](graphics/B04889_08_22.jpg)

在本例中，我们只在一个进程中运行应用程序，因此它将被分配到 CPU 的一个核心中。

在这种情况下，我们并没有利用运行应用程序的 CPU 的多核能力，但如果我们的算法出现一个异常，我们仍然可以获得自动重启应用程序的好处。

现在，我们将使用 CPU 中所有可用的内核来运行我们的应用程序，以便最大限度地利用它，但首先，我们需要停止集群:

```js
pm2 stop all

```

![Clustering Node.js applications](graphics/B04889_08_23.jpg)

PM2，停止所有服务后

```js
pm2 delete all

```

现在，我们可以使用 CPU 的所有核心重新运行应用程序了:

```js
pm2 start app.js -i 0

```

![Clustering Node.js applications](graphics/B04889_08_24.jpg)

PM2 显示以集群模式运行的四个服务

PM2 已经猜出了我们计算机的 cpu 数量，在我的例子中，这是一个四核的 iMac，如下截图所示:

![Clustering Node.js applications](graphics/B04889_08_25.jpg)

正如你在`pstree`中看到的，PM2 在操作系统级别启动了四个线程，如下图所示:

![Clustering Node.js applications](graphics/B04889_08_26.jpg)

当对一个应用程序进行集群化时，有一个关于应用程序应该使用的核数的不成文规则，这个数字是核数减去 1。

这个数字背后的原因是,操作系统需要一些 CPU,所以如果我们在我们的应用程序中使用所有的 CPU,一旦操作系统开始进行一些其他任务,它将迫使上下文切换的核心将是繁忙的,这将减缓应用程序。

# 负载均衡

有时，集群我们的应用是不够的，我们需要横向扩展我们的应用。

有很多方法可以水平扩展应用。如今，有了亚马逊(Amazon)这样的云提供商，每个提供商都已经实现了自己的解决方案，具有许多特性。

我喜欢的实现负载平衡的方法之一是使用**NGINX**。

NGINX 是一个 T0 web 服务器，它非常注重并发性和低内存的使用。 它也非常适合 Node.js 应用程序，因为它非常不鼓励从 Node.js 应用程序中提供静态资源。 这样做的主要原因是为了避免应用程序因为一个任务而承受压力，而这个任务本可以用其他软件更好地完成，比如 NGINX(这是专门化的另一个例子)。

但是，让我们关注负载平衡。 下图显示了 NGINX 作为负载均衡器的工作方式:

![Load balancing our application](graphics/B04889_08_27.jpg)

正如你在前面的图表中看到的，我们有两个PM2 集群由 NGINX 的一个实例负载平衡。

我们需要做的第一件事是知道 NGINX 如何管理配置。

在 Linux 上，NGINX 可以通过`yum`，`apt-get`或者其他的包管理器来安装。 它也可以从源代码构建，但是推荐的方法是使用包管理器，除非您有非常具体的需求。

默认情况下，主配置文件为`/etc/nginx/nginx.conf`，如下所示:

```js
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
  worker_connections  1024;
}

http {
  include                     /etc/nginx/mime.types;
  default_type                application/octet-stream;

  log_format          main '$remote_addr - $remote_user [$time_local] "$request" '
     '$status $body_bytes_sent "$http_referer" '
     '"$http_user_agent" "$http_x_forwarded_for" '
     '$request_time';

  access_log                  /var/log/nginx/access.log  main;
  server_tokens               off;
  sendfile                    on;
  #tcp_nopush                 on;
  keepalive_timeout           65s;
  send_timeout                15s;
  client_header_timeout       15s;
  client_body_timeout         15s;
  client_max_body_size        5m;
  ignore_invalid_headers      on;
  fastcgi_buffers             16 4k;
  #gzip                       on;
  include                     /etc/nginx/sites-enabled/*.conf;
}
```

这个文件非常简单，它指定工作人员的数量(记住，处理请求的进程)、错误日志的位置、工作人员当时可以激活的连接数量，最后是 HTTP 配置。

最后一行是最有趣的:我们通知 NGINX 使用`/etc/nginx/sites-enabled/*.conf`作为潜在的配置文件。

使用这个配置，指定文件夹下以`.conf`结尾的每个文件都将成为NGINX 配置的一部分。

如您所见，这里已经存在一个默认文件。 修改如下:

```js
http {
  upstream app {
    server 10.0.0.1:3000;
    server 10.0.0.2:3000;
  }
  server {
    listen 80;
    location / {
      proxy_pass http://app;
    }
  }
}
```

这就是构建负载均衡器所需的所有配置。 让我们在下面解释一下:

*   `upstream app`指令正在创建一组名为`app`的服务。 在这个指令中，我们指定了两个服务器，就像我们在前面的图片中看到的那样。
*   `server`指令指定 NGINX 应该监听来自`80`端口的所有请求，并将它们传递给名为`app`的上游组。

现在，NGINX 如何决定将请求发送到哪台计算机?

在本例中，我们可以指定用于分散负载的策略。

默认情况下，NGINX 在没有特别配置均衡方法时，使用**轮询**。

要记住的一件事是，如果我们使用轮询，我们的应用程序应该是无状态的，因为我们不会总是访问同一台机器，所以如果我们将状态保存在服务器上，它可能不在下面的调用中。

轮循是将工作队列中的负载分配给若干个工人的最基本方法; 它旋转它们，以便每个节点获得相同数量的请求*。*

 *还有其他分担负担的机制，如下:

```js
  upstream app {
    least_conn;
    server 10.0.0.1:3000;
    server 10.0.0.2:3000;
  }
```

**最小连接的**，正如其名称所示，将请求发送给最小连接的节点，在所有节点之间平均分配负载:

```js
  upstream app {
    ip_hash;
    server 10.0.0.1:3000;
    server 10.0.0.2:3000;
  }
```

**IP 哈希**是一种有趣的负载分配方式。 如果您曾经使用过任何 web 应用程序，那么会话的概念几乎存在于任何应用程序中。 为了记住用户是谁，浏览器向服务器发送一个 cookie，服务器在内存中存储了用户的身份以及他/她需要/可以被给定用户访问的数据。 另一种类型的负载平衡的问题是，我们不能保证总是访问同一个服务器。

的例子,如果我们使用至少连接作为平衡的政策,我们可以点击服务器的第一个负载,然后打了一个不同的服务器在随后的重定向,将导致用户不显示正确的信息作为第二个服务器不知道用户是谁。

使用 IP 哈希，负载均衡器将计算给定 IP 的哈希值。 这个哈希会导致数字从 1 到 N*,在*N*是服务器的数量,然后,用户总是会重定向到同一台机器,只要他们保持相同的 IP。*

我们还可以在负载均衡中应用一个权重，如下所示:

```js
  upstream app {
    server 10.0.0.1:3000 weight=5;
    server 10.0.0.2:3000;
  }
```

这将以这样的方式分配负载，每 6 个请求，5 个将被定向到第一台机器，1 个将被定向到第二台机器。

一旦我们选择了我们喜欢的负载平衡方法，我们可以重新启动 NGINX 使改变生效，但首先，我们想要验证它们，如下图所示:

![Load balancing our application](graphics/B04889_08_28.jpg)

正如您可以看到的，为了避免配置灾难，配置测试非常有用。

一旦 NGINX 通过`configtest`，保证 NGINX 能够`restart/start/reload`而没有任何语法问题，如下所示:

```js
sudo /etc/init.d/nginx reload

```

Reload 将优雅地等待，直到旧线程完成，然后，重新加载配置并使用新配置路由新请求。

如果你有兴趣了解 NGINX，我发现以下 NGINX 的官方文档非常有用:

[http://nginx.org/en/docs/](http://nginx.org/en/docs/)

## NGINX 运行状况检查

运行状况检查是负载平衡器上的重要活动之一。 如果其中一个节点发生了严重的硬件故障，无法为提供更多的请求，会发生什么情况?

在这种情况下，NGINX 自带两种类型的健康检查:**被动**和**主动**。

### 被动健康检查

这里，NGINX 被配置为一个反向代理(正如我们在前一节中所做的)。 它响应来自上游服务器的某种类型的响应。

如果有一个错误返回，NGINX 将标记该节点为故障，将其从负载平衡中移除一段时间，然后重新引入*。* 使用这种策略，NGINX 将不断地从负载均衡器中移除节点，从而大大减少故障的数量。

有一些可配置的参数，如`max_fails`或`fail_timeout`，我们可以在这些参数中配置将节点标记为无效所需的失败数量或请求超时时间。

### 当前运行状况检查

主动运行状况检查与被动运行状况检查不同，主动向上游服务器发出连接，以检查它们是否正确响应遇到的问题。

NGINX 中主动健康检查的最简单配置如下:

```js
http {
  upstream app {
    zone app test;
    server 10.0.0.1:3000;
    server 10.0.0.2:3000;
  }
  server {
    listen 80;
    location / {
      proxy_pass http://app;
      health_check;
    }
  }
}
```

在这个配置文件中有两个新行，如下所示:

*   `health_check`:启用当前运行状况检查。 默认配置是每 5 秒向`upstream`节中指定的主机和端口发出一个连接。
*   `zone app test`:启用健康检查时 NGINX 配置要求。

有很多选项可以配置更具体的健康检查，所有这些选项都可以在 NGINX 配置中使用，可以组合起来满足不同用户的需求。

# 小结

在本章中，您学习了用于部署微服务的广泛技术。 到目前为止，您已经了解了如何构建、部署和配置软件组件，从而能够使各种技术均质化。 本书的目的是向您提供开始使用微服务所需的概念，并使读者知道如何查找所需的信息。

就我个人而言，我一直在努力寻找一本书来总结微服务生命周期的所有方面，我真的希望这本书能涵盖这一空白领域。***